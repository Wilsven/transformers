{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_word(text, space_token=\"_\"):\n",
    "    return \" \".join(list(text)) + \" \" + space_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vocab(corpus):\n",
    "    text = re.sub(\"\\s+\", \" \", corpus.lower())\n",
    "    all_words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    vocab = {}\n",
    "    for word in all_words:\n",
    "        word = format_word(word)\n",
    "        vocab[word] = vocab.get(word, 0) + 1\n",
    "    tokens = collections.Counter(text)\n",
    "    vocab = dict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))\n",
    "    \n",
    "    return vocab, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text split: ['hi', ',', 'my', 'name', 'is', 'john', 'and', 'i', 'love', 'dog', '.', 'his', 'name', 'is', 'also', 'john', 'which', 'is', 'short', 'for', 'johnny', 'and', 'he', 'loves', 'not', 'only', 'dogs', 'but', 'all', 'kinds', 'of', 'animals', 'as', 'well', '.', 'her', 'name', 'is', 'jane', 'and', 'it', 'is', 'funny', 'how', 'our', 'names', 'all', 'start', 'with', 'the', 'letter', 'j', 'and', 'she', 'loves', 'cats', '.', 'she', 'is', 'afraid', 'of', 'dogs', '.']\n",
      "Before format: hi\n",
      "After format: h i _\n",
      "Before format: ,\n",
      "After format: , _\n",
      "Before format: my\n",
      "After format: m y _\n",
      "Before format: name\n",
      "After format: n a m e _\n",
      "Before format: is\n",
      "After format: i s _\n",
      "Before format: john\n",
      "After format: j o h n _\n",
      "Before format: and\n",
      "After format: a n d _\n",
      "Before format: i\n",
      "After format: i _\n",
      "Before format: love\n",
      "After format: l o v e _\n",
      "Before format: dog\n",
      "After format: d o g _\n",
      "Before format: .\n",
      "After format: . _\n",
      "Before format: his\n",
      "After format: h i s _\n",
      "Before format: name\n",
      "After format: n a m e _\n",
      "Before format: is\n",
      "After format: i s _\n",
      "Before format: also\n",
      "After format: a l s o _\n",
      "Before format: john\n",
      "After format: j o h n _\n",
      "Before format: which\n",
      "After format: w h i c h _\n",
      "Before format: is\n",
      "After format: i s _\n",
      "Before format: short\n",
      "After format: s h o r t _\n",
      "Before format: for\n",
      "After format: f o r _\n",
      "Before format: johnny\n",
      "After format: j o h n n y _\n",
      "Before format: and\n",
      "After format: a n d _\n",
      "Before format: he\n",
      "After format: h e _\n",
      "Before format: loves\n",
      "After format: l o v e s _\n",
      "Before format: not\n",
      "After format: n o t _\n",
      "Before format: only\n",
      "After format: o n l y _\n",
      "Before format: dogs\n",
      "After format: d o g s _\n",
      "Before format: but\n",
      "After format: b u t _\n",
      "Before format: all\n",
      "After format: a l l _\n",
      "Before format: kinds\n",
      "After format: k i n d s _\n",
      "Before format: of\n",
      "After format: o f _\n",
      "Before format: animals\n",
      "After format: a n i m a l s _\n",
      "Before format: as\n",
      "After format: a s _\n",
      "Before format: well\n",
      "After format: w e l l _\n",
      "Before format: .\n",
      "After format: . _\n",
      "Before format: her\n",
      "After format: h e r _\n",
      "Before format: name\n",
      "After format: n a m e _\n",
      "Before format: is\n",
      "After format: i s _\n",
      "Before format: jane\n",
      "After format: j a n e _\n",
      "Before format: and\n",
      "After format: a n d _\n",
      "Before format: it\n",
      "After format: i t _\n",
      "Before format: is\n",
      "After format: i s _\n",
      "Before format: funny\n",
      "After format: f u n n y _\n",
      "Before format: how\n",
      "After format: h o w _\n",
      "Before format: our\n",
      "After format: o u r _\n",
      "Before format: names\n",
      "After format: n a m e s _\n",
      "Before format: all\n",
      "After format: a l l _\n",
      "Before format: start\n",
      "After format: s t a r t _\n",
      "Before format: with\n",
      "After format: w i t h _\n",
      "Before format: the\n",
      "After format: t h e _\n",
      "Before format: letter\n",
      "After format: l e t t e r _\n",
      "Before format: j\n",
      "After format: j _\n",
      "Before format: and\n",
      "After format: a n d _\n",
      "Before format: she\n",
      "After format: s h e _\n",
      "Before format: loves\n",
      "After format: l o v e s _\n",
      "Before format: cats\n",
      "After format: c a t s _\n",
      "Before format: .\n",
      "After format: . _\n",
      "Before format: she\n",
      "After format: s h e _\n",
      "Before format: is\n",
      "After format: i s _\n",
      "Before format: afraid\n",
      "After format: a f r a i d _\n",
      "Before format: of\n",
      "After format: o f _\n",
      "Before format: dogs\n",
      "After format: d o g s _\n",
      "Before format: .\n",
      "After format: . _\n"
     ]
    }
   ],
   "source": [
    "corpus = \"\"\"\n",
    "Hi, my name is John and I love dog. His name is also John which is short for Johnny and he loves not only dogs but all kinds of animals as well.\n",
    "Her name is Jane and it is funny how our names all start with the letter J and she loves cats. She is afraid of dogs.\n",
    "\"\"\"\n",
    "\n",
    "text = re.sub(\"\\s+\", \" \", corpus.lower())\n",
    "all_words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "print(\"Text split:\", all_words)\n",
    "\n",
    "vocab = {}\n",
    "for word in all_words:\n",
    "    print(\"Before format:\", word)\n",
    "    word = format_word(word)\n",
    "    print(\"After format:\", word)\n",
    "    vocab[word] = vocab.get(word, 0) + 1\n",
    "\n",
    "vocab = dict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))\n",
    "tokens = collections.Counter(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i s _': 6,\n",
       " 'a n d _': 4,\n",
       " '. _': 4,\n",
       " 'n a m e _': 3,\n",
       " 'j o h n _': 2,\n",
       " 'l o v e s _': 2,\n",
       " 'd o g s _': 2,\n",
       " 'a l l _': 2,\n",
       " 'o f _': 2,\n",
       " 's h e _': 2,\n",
       " 'h i _': 1,\n",
       " ', _': 1,\n",
       " 'm y _': 1,\n",
       " 'i _': 1,\n",
       " 'l o v e _': 1,\n",
       " 'd o g _': 1,\n",
       " 'h i s _': 1,\n",
       " 'a l s o _': 1,\n",
       " 'w h i c h _': 1,\n",
       " 's h o r t _': 1,\n",
       " 'f o r _': 1,\n",
       " 'j o h n n y _': 1,\n",
       " 'h e _': 1,\n",
       " 'n o t _': 1,\n",
       " 'o n l y _': 1,\n",
       " 'b u t _': 1,\n",
       " 'k i n d s _': 1,\n",
       " 'a n i m a l s _': 1,\n",
       " 'a s _': 1,\n",
       " 'w e l l _': 1,\n",
       " 'h e r _': 1,\n",
       " 'j a n e _': 1,\n",
       " 'i t _': 1,\n",
       " 'f u n n y _': 1,\n",
       " 'h o w _': 1,\n",
       " 'o u r _': 1,\n",
       " 'n a m e s _': 1,\n",
       " 's t a r t _': 1,\n",
       " 'w i t h _': 1,\n",
       " 't h e _': 1,\n",
       " 'l e t t e r _': 1,\n",
       " 'j _': 1,\n",
       " 'c a t s _': 1,\n",
       " 'a f r a i d _': 1}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' ': 59,\n",
       "         's': 21,\n",
       "         'n': 19,\n",
       "         'a': 19,\n",
       "         'o': 18,\n",
       "         'e': 16,\n",
       "         'h': 15,\n",
       "         'i': 15,\n",
       "         'l': 13,\n",
       "         't': 11,\n",
       "         'd': 9,\n",
       "         'r': 7,\n",
       "         'm': 6,\n",
       "         'j': 5,\n",
       "         'f': 5,\n",
       "         'y': 4,\n",
       "         '.': 4,\n",
       "         'w': 4,\n",
       "         'v': 3,\n",
       "         'g': 3,\n",
       "         'u': 3,\n",
       "         'c': 2,\n",
       "         ',': 1,\n",
       "         'b': 1,\n",
       "         'k': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_counts(vocab):\n",
    "    pairs = {}\n",
    "    for word, count in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pair = (symbols[i], symbols[i + 1])\n",
    "            pairs[pair] = pairs.get(pair, 0) + count\n",
    "    pairs = dict(sorted(pairs.items(), key=lambda x: x[1], reverse=True))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('s', '_'): 16,\n",
       " ('e', '_'): 9,\n",
       " ('i', 's'): 7,\n",
       " ('a', 'n'): 6,\n",
       " ('n', 'd'): 5,\n",
       " ('d', '_'): 5,\n",
       " ('h', 'e'): 5,\n",
       " ('t', '_'): 5,\n",
       " ('.', '_'): 4,\n",
       " ('n', 'a'): 4,\n",
       " ('a', 'm'): 4,\n",
       " ('m', 'e'): 4,\n",
       " ('a', 'l'): 4,\n",
       " ('y', '_'): 4,\n",
       " ('r', '_'): 4,\n",
       " ('j', 'o'): 3,\n",
       " ('o', 'h'): 3,\n",
       " ('h', 'n'): 3,\n",
       " ('l', 'o'): 3,\n",
       " ('o', 'v'): 3,\n",
       " ('v', 'e'): 3,\n",
       " ('e', 's'): 3,\n",
       " ('d', 'o'): 3,\n",
       " ('o', 'g'): 3,\n",
       " ('l', 'l'): 3,\n",
       " ('l', '_'): 3,\n",
       " ('s', 'h'): 3,\n",
       " ('h', 'i'): 3,\n",
       " ('n', '_'): 2,\n",
       " ('g', 's'): 2,\n",
       " ('o', 'f'): 2,\n",
       " ('f', '_'): 2,\n",
       " ('i', '_'): 2,\n",
       " ('l', 's'): 2,\n",
       " ('h', '_'): 2,\n",
       " ('h', 'o'): 2,\n",
       " ('o', 'r'): 2,\n",
       " ('r', 't'): 2,\n",
       " ('n', 'n'): 2,\n",
       " ('n', 'y'): 2,\n",
       " ('e', 'r'): 2,\n",
       " ('i', 't'): 2,\n",
       " ('t', 'h'): 2,\n",
       " (',', '_'): 1,\n",
       " ('m', 'y'): 1,\n",
       " ('g', '_'): 1,\n",
       " ('s', 'o'): 1,\n",
       " ('o', '_'): 1,\n",
       " ('w', 'h'): 1,\n",
       " ('i', 'c'): 1,\n",
       " ('c', 'h'): 1,\n",
       " ('f', 'o'): 1,\n",
       " ('n', 'o'): 1,\n",
       " ('o', 't'): 1,\n",
       " ('o', 'n'): 1,\n",
       " ('n', 'l'): 1,\n",
       " ('l', 'y'): 1,\n",
       " ('b', 'u'): 1,\n",
       " ('u', 't'): 1,\n",
       " ('k', 'i'): 1,\n",
       " ('i', 'n'): 1,\n",
       " ('d', 's'): 1,\n",
       " ('n', 'i'): 1,\n",
       " ('i', 'm'): 1,\n",
       " ('m', 'a'): 1,\n",
       " ('a', 's'): 1,\n",
       " ('w', 'e'): 1,\n",
       " ('e', 'l'): 1,\n",
       " ('j', 'a'): 1,\n",
       " ('n', 'e'): 1,\n",
       " ('f', 'u'): 1,\n",
       " ('u', 'n'): 1,\n",
       " ('o', 'w'): 1,\n",
       " ('w', '_'): 1,\n",
       " ('o', 'u'): 1,\n",
       " ('u', 'r'): 1,\n",
       " ('s', 't'): 1,\n",
       " ('t', 'a'): 1,\n",
       " ('a', 'r'): 1,\n",
       " ('w', 'i'): 1,\n",
       " ('l', 'e'): 1,\n",
       " ('e', 't'): 1,\n",
       " ('t', 't'): 1,\n",
       " ('t', 'e'): 1,\n",
       " ('j', '_'): 1,\n",
       " ('c', 'a'): 1,\n",
       " ('a', 't'): 1,\n",
       " ('t', 's'): 1,\n",
       " ('a', 'f'): 1,\n",
       " ('f', 'r'): 1,\n",
       " ('r', 'a'): 1,\n",
       " ('a', 'i'): 1,\n",
       " ('i', 'd'): 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = {}\n",
    "for word, count in vocab.items():\n",
    "    symbols = word.split()\n",
    "    # Iterate up to the second last word because we are\n",
    "    # sliding across the text with a window size of 2\n",
    "    for i in range(len(symbols) - 1):\n",
    "        pair = (symbols[i], symbols[i + 1])  # get bigram\n",
    "        pairs[pair] = pairs.get(pair, 0) + count\n",
    "\n",
    "# Sort based on the counts\n",
    "pairs = dict(sorted(pairs.items(), key=lambda x: x[1], reverse=True))\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_vocab(pair, vocab_in):\n",
    "    vocab_out = {}\n",
    "    bigram = re.escape(\" \".join(pair))\n",
    "    p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
    "    bytepair = \"\".join(pair)\n",
    "    for word in vocab_in:\n",
    "        w_out = p.sub(bytepair, word)\n",
    "        vocab_out[w_out] = vocab_in[word]\n",
    "    return vocab_out, (bigram, bytepair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_merges(vocab, tokens, num_merges):\n",
    "    merges = []\n",
    "    for _ in range(num_merges):\n",
    "        pairs = get_bigram_counts(vocab)\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        best_count = pairs[best_pair]\n",
    "        vocab, (bigram, bytepair) = merge_vocab(best_pair, vocab)\n",
    "        merges.append((r\"(?<!\\S)\" + bigram + r\"(?!\\S)\", bytepair))\n",
    "        tokens[bytepair] = best_count\n",
    "    return vocab, tokens, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best pair: ('s', '_')\n",
      "Best count: 16\n"
     ]
    }
   ],
   "source": [
    "best_pair = max(pairs, key=pairs.get)\n",
    "print(\"Best pair:\", best_pair)\n",
    "best_count = pairs[best_pair]\n",
    "print(\"Best count:\", best_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram: s\\ _\n",
      "p: re.compile('(?<!\\\\S)s\\\\ _(?!\\\\S)')\n",
      "bytepair: s_\n"
     ]
    }
   ],
   "source": [
    "bigram = re.escape(\" \".join(best_pair))\n",
    "print(\"Bigram:\", bigram)\n",
    "# Negative lookbehind -> r\"(?<!\\S)\" -> What's behind that is NOT a character (whitespaces are okay)?\n",
    "# Negative lookahead ->  r\"(?!\\S)\" -> What's ahead that is NOT a character (whitespaces are okay)?\n",
    "# Basically, the negative lookbehind and lookahead regexes are looking for bigrams isolated by spaces before and after\n",
    "p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
    "print(\"p:\", p)\n",
    "bytepair = \"\".join(best_pair)  # merge rule\n",
    "print(\"bytepair:\", bytepair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "word: i s _\n",
      "w_out: i s_\n",
      "---\n",
      "\n",
      "word: a n d _\n",
      "w_out: a n d _\n",
      "---\n",
      "\n",
      "word: . _\n",
      "w_out: . _\n",
      "---\n",
      "\n",
      "word: n a m e _\n",
      "w_out: n a m e _\n",
      "---\n",
      "\n",
      "word: j o h n _\n",
      "w_out: j o h n _\n",
      "---\n",
      "\n",
      "word: l o v e s _\n",
      "w_out: l o v e s_\n",
      "---\n",
      "\n",
      "word: d o g s _\n",
      "w_out: d o g s_\n",
      "---\n",
      "\n",
      "word: a l l _\n",
      "w_out: a l l _\n",
      "---\n",
      "\n",
      "word: o f _\n",
      "w_out: o f _\n",
      "---\n",
      "\n",
      "word: s h e _\n",
      "w_out: s h e _\n",
      "---\n",
      "\n",
      "word: h i _\n",
      "w_out: h i _\n",
      "---\n",
      "\n",
      "word: , _\n",
      "w_out: , _\n",
      "---\n",
      "\n",
      "word: m y _\n",
      "w_out: m y _\n",
      "---\n",
      "\n",
      "word: i _\n",
      "w_out: i _\n",
      "---\n",
      "\n",
      "word: l o v e _\n",
      "w_out: l o v e _\n",
      "---\n",
      "\n",
      "word: d o g _\n",
      "w_out: d o g _\n",
      "---\n",
      "\n",
      "word: h i s _\n",
      "w_out: h i s_\n",
      "---\n",
      "\n",
      "word: a l s o _\n",
      "w_out: a l s o _\n",
      "---\n",
      "\n",
      "word: w h i c h _\n",
      "w_out: w h i c h _\n",
      "---\n",
      "\n",
      "word: s h o r t _\n",
      "w_out: s h o r t _\n",
      "---\n",
      "\n",
      "word: f o r _\n",
      "w_out: f o r _\n",
      "---\n",
      "\n",
      "word: j o h n n y _\n",
      "w_out: j o h n n y _\n",
      "---\n",
      "\n",
      "word: h e _\n",
      "w_out: h e _\n",
      "---\n",
      "\n",
      "word: n o t _\n",
      "w_out: n o t _\n",
      "---\n",
      "\n",
      "word: o n l y _\n",
      "w_out: o n l y _\n",
      "---\n",
      "\n",
      "word: b u t _\n",
      "w_out: b u t _\n",
      "---\n",
      "\n",
      "word: k i n d s _\n",
      "w_out: k i n d s_\n",
      "---\n",
      "\n",
      "word: a n i m a l s _\n",
      "w_out: a n i m a l s_\n",
      "---\n",
      "\n",
      "word: a s _\n",
      "w_out: a s_\n",
      "---\n",
      "\n",
      "word: w e l l _\n",
      "w_out: w e l l _\n",
      "---\n",
      "\n",
      "word: h e r _\n",
      "w_out: h e r _\n",
      "---\n",
      "\n",
      "word: j a n e _\n",
      "w_out: j a n e _\n",
      "---\n",
      "\n",
      "word: i t _\n",
      "w_out: i t _\n",
      "---\n",
      "\n",
      "word: f u n n y _\n",
      "w_out: f u n n y _\n",
      "---\n",
      "\n",
      "word: h o w _\n",
      "w_out: h o w _\n",
      "---\n",
      "\n",
      "word: o u r _\n",
      "w_out: o u r _\n",
      "---\n",
      "\n",
      "word: n a m e s _\n",
      "w_out: n a m e s_\n",
      "---\n",
      "\n",
      "word: s t a r t _\n",
      "w_out: s t a r t _\n",
      "---\n",
      "\n",
      "word: w i t h _\n",
      "w_out: w i t h _\n",
      "---\n",
      "\n",
      "word: t h e _\n",
      "w_out: t h e _\n",
      "---\n",
      "\n",
      "word: l e t t e r _\n",
      "w_out: l e t t e r _\n",
      "---\n",
      "\n",
      "word: j _\n",
      "w_out: j _\n",
      "---\n",
      "\n",
      "word: c a t s _\n",
      "w_out: c a t s_\n",
      "---\n",
      "\n",
      "word: a f r a i d _\n",
      "w_out: a f r a i d _\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "vocab_out = {}\n",
    "for word in vocab:\n",
    "    # Subsitute the part in the word with the bytepair\n",
    "    # if there is a part in the word that matches the regex criteria `p`\n",
    "    w_out = p.sub(bytepair, word)\n",
    "    print(\"\")\n",
    "    print(\"word:\", word)\n",
    "    print(\"w_out:\", w_out)\n",
    "    print(\"---\")\n",
    "    vocab_out[w_out] = vocab[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i s_': 6,\n",
       " 'a n d _': 4,\n",
       " '. _': 4,\n",
       " 'n a m e _': 3,\n",
       " 'j o h n _': 2,\n",
       " 'l o v e s_': 2,\n",
       " 'd o g s_': 2,\n",
       " 'a l l _': 2,\n",
       " 'o f _': 2,\n",
       " 's h e _': 2,\n",
       " 'h i _': 1,\n",
       " ', _': 1,\n",
       " 'm y _': 1,\n",
       " 'i _': 1,\n",
       " 'l o v e _': 1,\n",
       " 'd o g _': 1,\n",
       " 'h i s_': 1,\n",
       " 'a l s o _': 1,\n",
       " 'w h i c h _': 1,\n",
       " 's h o r t _': 1,\n",
       " 'f o r _': 1,\n",
       " 'j o h n n y _': 1,\n",
       " 'h e _': 1,\n",
       " 'n o t _': 1,\n",
       " 'o n l y _': 1,\n",
       " 'b u t _': 1,\n",
       " 'k i n d s_': 1,\n",
       " 'a n i m a l s_': 1,\n",
       " 'a s_': 1,\n",
       " 'w e l l _': 1,\n",
       " 'h e r _': 1,\n",
       " 'j a n e _': 1,\n",
       " 'i t _': 1,\n",
       " 'f u n n y _': 1,\n",
       " 'h o w _': 1,\n",
       " 'o u r _': 1,\n",
       " 'n a m e s_': 1,\n",
       " 's t a r t _': 1,\n",
       " 'w i t h _': 1,\n",
       " 't h e _': 1,\n",
       " 'l e t t e r _': 1,\n",
       " 'j _': 1,\n",
       " 'c a t s_': 1,\n",
       " 'a f r a i d _': 1}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = []\n",
    "merges.append((r\"(?<!\\S)\" + bigram + r\"(?!\\S)\", bytepair))  # store the merge rule\n",
    "tokens[bytepair] = best_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(?<!\\\\S)s\\\\ _(?!\\\\S)', 's_')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' ': 59,\n",
       "         's': 21,\n",
       "         'n': 19,\n",
       "         'a': 19,\n",
       "         'o': 18,\n",
       "         'e': 16,\n",
       "         's_': 16,\n",
       "         'h': 15,\n",
       "         'i': 15,\n",
       "         'l': 13,\n",
       "         't': 11,\n",
       "         'd': 9,\n",
       "         'r': 7,\n",
       "         'm': 6,\n",
       "         'j': 5,\n",
       "         'f': 5,\n",
       "         'y': 4,\n",
       "         '.': 4,\n",
       "         'w': 4,\n",
       "         'v': 3,\n",
       "         'g': 3,\n",
       "         'u': 3,\n",
       "         'c': 2,\n",
       "         ',': 1,\n",
       "         'b': 1,\n",
       "         'k': 1})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(text, num_merges):\n",
    "    vocab, tokens = initialize_vocab(text)\n",
    "    characters = set(tokens.keys())\n",
    "    vocab, tokens, merges = find_merges(vocab, tokens, num_merges)\n",
    "\n",
    "    return characters, vocab, tokens, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters, vocab, tokens, merges = fit(text, num_merges=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ',\n",
       " ',',\n",
       " '.',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'y'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_': 6,\n",
       " 'and_': 4,\n",
       " '._': 4,\n",
       " 'nam e_': 3,\n",
       " 'j o h n _': 2,\n",
       " 'l o v e s_': 2,\n",
       " 'd o g s_': 2,\n",
       " 'a l l _': 2,\n",
       " 'o f _': 2,\n",
       " 's h e_': 2,\n",
       " 'h i _': 1,\n",
       " ', _': 1,\n",
       " 'm y _': 1,\n",
       " 'i _': 1,\n",
       " 'l o v e_': 1,\n",
       " 'd o g _': 1,\n",
       " 'h is_': 1,\n",
       " 'a l s o _': 1,\n",
       " 'w h i c h _': 1,\n",
       " 's h o r t_': 1,\n",
       " 'f o r _': 1,\n",
       " 'j o h n n y _': 1,\n",
       " 'h e_': 1,\n",
       " 'n o t_': 1,\n",
       " 'o n l y _': 1,\n",
       " 'b u t_': 1,\n",
       " 'k i n d s_': 1,\n",
       " 'an i m a l s_': 1,\n",
       " 'a s_': 1,\n",
       " 'w e l l _': 1,\n",
       " 'h e r _': 1,\n",
       " 'j an e_': 1,\n",
       " 'i t_': 1,\n",
       " 'f u n n y _': 1,\n",
       " 'h o w _': 1,\n",
       " 'o u r _': 1,\n",
       " 'nam e s_': 1,\n",
       " 's t a r t_': 1,\n",
       " 'w i t h _': 1,\n",
       " 't h e_': 1,\n",
       " 'l e t t e r _': 1,\n",
       " 'j _': 1,\n",
       " 'c a t s_': 1,\n",
       " 'a f r a i d_': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({' ': 59,\n",
       "         's': 21,\n",
       "         'n': 19,\n",
       "         'a': 19,\n",
       "         'o': 18,\n",
       "         'e': 16,\n",
       "         's_': 16,\n",
       "         'h': 15,\n",
       "         'i': 15,\n",
       "         'l': 13,\n",
       "         't': 11,\n",
       "         'd': 9,\n",
       "         'e_': 9,\n",
       "         'r': 7,\n",
       "         'is_': 7,\n",
       "         'm': 6,\n",
       "         'an': 6,\n",
       "         'j': 5,\n",
       "         'f': 5,\n",
       "         'd_': 5,\n",
       "         't_': 5,\n",
       "         'y': 4,\n",
       "         '.': 4,\n",
       "         'w': 4,\n",
       "         'and_': 4,\n",
       "         '._': 4,\n",
       "         'na': 4,\n",
       "         'nam': 4,\n",
       "         'v': 3,\n",
       "         'g': 3,\n",
       "         'u': 3,\n",
       "         'c': 2,\n",
       "         ',': 1,\n",
       "         'b': 1,\n",
       "         'k': 1})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('(?<!\\\\S)s\\\\ _(?!\\\\S)', 's_'),\n",
       " ('(?<!\\\\S)e\\\\ _(?!\\\\S)', 'e_'),\n",
       " ('(?<!\\\\S)i\\\\ s_(?!\\\\S)', 'is_'),\n",
       " ('(?<!\\\\S)a\\\\ n(?!\\\\S)', 'an'),\n",
       " ('(?<!\\\\S)d\\\\ _(?!\\\\S)', 'd_'),\n",
       " ('(?<!\\\\S)t\\\\ _(?!\\\\S)', 't_'),\n",
       " ('(?<!\\\\S)an\\\\ d_(?!\\\\S)', 'and_'),\n",
       " ('(?<!\\\\S)\\\\.\\\\ _(?!\\\\S)', '._'),\n",
       " ('(?<!\\\\S)n\\\\ a(?!\\\\S)', 'na'),\n",
       " ('(?<!\\\\S)na\\\\ m(?!\\\\S)', 'nam')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncoding:\n",
    "    def __init__(self):\n",
    "        self.characters = None\n",
    "        self.vocab = None\n",
    "        self.tokens = None\n",
    "        self.merges = None\n",
    "        \n",
    "    def format_word(self, text, space_token=\"_\"):\n",
    "        return \" \".join(list(text)) + \" \" + space_token\n",
    "    \n",
    "    def initialize_vocab(self, corpus):\n",
    "        text = re.sub(\"\\s+\", \" \", corpus.lower())\n",
    "        all_words = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "        vocab = {}\n",
    "        for word in all_words:\n",
    "            word = self.format_word(word)\n",
    "            vocab[word] = vocab.get(word, 0) + 1\n",
    "        tokens = collections.Counter(text)\n",
    "        vocab = dict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))\n",
    "        \n",
    "        return vocab, tokens\n",
    "    \n",
    "    def get_bigram_counts(self, vocab):\n",
    "        pairs = {}\n",
    "        for word, count in vocab.items():\n",
    "            symbols = word.split()\n",
    "            for i in range(len(symbols) - 1):\n",
    "                pair = (symbols[i], symbols[i + 1])\n",
    "                pairs[pair] = pairs.get(pair, 0) + count\n",
    "        pairs = dict(sorted(pairs.items(), key=lambda x: x[1], reverse=True))\n",
    "        \n",
    "        return pairs\n",
    "    \n",
    "    def merge_vocab(self, pair, vocab_in):\n",
    "        vocab_out = {}\n",
    "        bigram = re.escape(\" \".join(pair))\n",
    "        p = re.compile(r\"(?<!\\S)\" + bigram + r\"(?!\\S)\")\n",
    "        bytepair = \"\".join(pair)\n",
    "        for word in vocab_in:\n",
    "            w_out = p.sub(bytepair, word)\n",
    "            vocab_out[w_out] = vocab_in[word]\n",
    "            \n",
    "        return vocab_out, (bigram, bytepair)\n",
    "    \n",
    "    def find_merges(self, vocab, tokens, num_merges):\n",
    "        merges = []\n",
    "        for _ in range(num_merges):\n",
    "            pairs = self.get_bigram_counts(vocab)\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            best_count = pairs[best_pair]\n",
    "            vocab, (bigram, bytepair) = self.merge_vocab(best_pair, vocab)\n",
    "            merges.append((r\"(?<!\\S)\" + bigram + r\"(?!\\S)\", bytepair))\n",
    "            tokens[bytepair] = best_count\n",
    "            \n",
    "        return vocab, tokens, merges\n",
    "    \n",
    "    def fit(self, text, num_merges):\n",
    "        vocab, tokens = self.initialize_vocab(text)\n",
    "        self.characters = set(tokens.keys())\n",
    "        self.vocab, self.tokens, self.merges = self.find_merges(vocab, tokens, num_merges)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
