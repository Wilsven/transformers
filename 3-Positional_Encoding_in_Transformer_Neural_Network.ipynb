{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoding in Transformer Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer-architecture-2.png\" width=\"500\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Neural Network Architecture Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's walk through exactly how the inital part of the Transformer Neural Network architecture works so that it kind of motivates positional encodings better.\n",
    "\n",
    "Firstly, we have an input sequence in English that is, *My name is John*. Typically, the way that Transformers and all these machine learning models work is that they understand numbers, vectors and matrices but they don't exactly understand words. So, what we would do is make sure to pass in a fixed length matrix by making sure we pad all the words that are not present — usually with a dummy character or dummy sequence input. This would be the maximum number of words allowed in an input sequence which would go into the Transformer.\n",
    "\n",
    "The words are then one-hot encoded. The vocabulary size is the number of words in our dictionary — that is the number of possible words which can be used in the input sequence.\n",
    "\n",
    "We now pass the input sequence into a feed forward layer where each of the vectors in the input sequence will be mapped to a 512-dimensional vector and the parameters, $W_{E}$ are learnable via back propagation. The total number of learning parameters in this feed forward layer will be the vocabulary size times 512.\n",
    "\n",
    "The output of this feed forward layer would simply the a matrix size of maximum input sequence length times 512. We'll refer to this matrix as $X$. It is to this matrix $X$ that we are going to add some positional encoding which is of the same size. In doing so, we are going to get another matrix, $X^{1}$. \n",
    "\n",
    "For each word vector in matrix $X^{1}$, we want to generate a query, key and value matrix — all of 512 dimensions each. We accomplish this by passing each vecor in matrix $X^{1}$ into a set of query, key and value weights — $W_{Q}$, $W_{K}$ and $W_{V}$ respectively. This operation would map one input vector to output a query, key and value matrix. We do this for every single word of the input sequence up to the maximum sequence length.\n",
    "\n",
    "The number of total output vectors would be 3 times the maximum sequence length because we get 3 matrices — $Q$, $K$ and $V$ — for every single word. From this point, we can probably split these output vectors into multiple heads and perform [multi head attention](2-Multihead_Attention.ipynb).\n",
    "\n",
    "**Note:** The query, key and value weights — $W_{Q}$, $W_{K}$ and $W_{V}$ — are learnable parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![diagram](images/positional-encoding.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Embedding\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "PE(\\text{position}, 2i) = \\sin{(\\frac{\\text{position}}{10000^{\\frac{2i}{d_{model}}}})} \\newline\n",
    "\n",
    "\\nonumber\\newline\n",
    "\n",
    "PE(\\text{position}, 2i + 1) = \\cos{(\\frac{\\text{position}}{10000^{\\frac{2i}{d_{model}}}})}\n",
    " \n",
    "\\end{align}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we formulate positional embedding this way? \n",
    "\n",
    "#### 1. Periodicity \n",
    "\n",
    "Sine and cosine functions are periodic functions — so they will repeat after some point of time. For example, let's say we want to look at a particular vector — the third vecor, which would correspond to the word *is* — in the $X$ matrix and positional encoding matrix (see figure above). \n",
    "\n",
    "At some point, we are going to compute the attention matrix and try to determine how much attention the word *is* should pay attention to all other words. Now during this phase, because of periodicity, the word *is* will be able to pay attention to say, five words after it and then 10 words after it, 15 words after it, in a much more tractable way.\n",
    "\n",
    "#### 2. Constrained Values\n",
    "\n",
    "Sine and cosine will constrain the values to be between -1 and 1. Without that, the values, at least in the positive direction, are not bounded. This would mean that positional encoding for the word *is* might be smaller than the next vector, which will be smaller than the next vector and so on. During the time when we compute the attention matrices, you'll find that the third vector — corresponding to the word *is* — is not going to be able to attend to vectors very far away from it. Therefore, it will not be able to derive any context from them.\n",
    "\n",
    "#### 3. Easy to Extrapolate Long Sequences\n",
    "\n",
    "Sine and consine are very deterministic formulas — very easy to compute. Even if the model hasn't seen certain sequence lengths in the training set, we'll still be able to interpret them in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `max_sequence_length` is the maximum number of words that can be passed into the transformer simultaneously.\n",
    "\n",
    "`d_model` is the dimension of the embeddings. It's typically 512 but for illustrative purposes, 6 is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_length = 10\n",
    "d_model = 6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the above equations $(1)$ and $(2)$ as:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "PE(\\text{position}, i) = \\sin{(\\frac{\\text{position}}{10000^{\\frac{i}{d_{model}}}})} \\text{ when $i$ is even} \\nonumber\n",
    "\n",
    "\\newline\\nonumber\n",
    "\\newline\\nonumber\n",
    "\n",
    "PE(\\text{position}, i) = \\cos{(\\frac{\\text{position}}{10000^{\\frac{i-1}{d_{model}}}})} \\text{ when $i$ is odd} \\nonumber\n",
    " \n",
    "\\end{align}\n",
    "\n",
    "The new equations now are easier to see and program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_i = torch.arange(0, d_model, 2).float()\n",
    "even_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,  21.5443, 464.1590])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_denominator = torch.pow(10000, even_i / d_model)\n",
    "even_denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 3., 5.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_i = torch.arange(1, d_model, 2).float()\n",
    "odd_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0000,  21.5443, 464.1590])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_denominator = torch.pow(10000, (odd_i - 1) / d_model)\n",
    "odd_denominator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you will notice is that the values of the `even_denominator` and `odd_denominator` are exactly the same. This makes sense because the odd indices are 1 more than the even indices and substracting 1 from the odd indices result in the same even indices. \n",
    "\n",
    "So, we are going to just combine the denominators into one `denominator` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = even_denominator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all the positions by taking the values from 0 to 9 and then reshape to be a 2-dimensional matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.],\n",
       "        [5.],\n",
       "        [6.],\n",
       "        [7.],\n",
       "        [8.],\n",
       "        [9.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position = torch.arange(max_sequence_length, dtype=float).reshape(\n",
    "    max_sequence_length, 1\n",
    ")\n",
    "\n",
    "print(position.shape)\n",
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "even_PE = torch.sin(position / denominator)\n",
    "odd_PE = torch.cos(position / denominator)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculation will result in a 10 times 3 matrix for both the even and odd cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000],\n",
       "        [ 0.8415,  0.0464,  0.0022],\n",
       "        [ 0.9093,  0.0927,  0.0043],\n",
       "        [ 0.1411,  0.1388,  0.0065],\n",
       "        [-0.7568,  0.1846,  0.0086],\n",
       "        [-0.9589,  0.2300,  0.0108],\n",
       "        [-0.2794,  0.2749,  0.0129],\n",
       "        [ 0.6570,  0.3192,  0.0151],\n",
       "        [ 0.9894,  0.3629,  0.0172],\n",
       "        [ 0.4121,  0.4057,  0.0194]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(even_PE.shape)\n",
    "even_PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000,  1.0000,  1.0000],\n",
       "        [ 0.5403,  0.9989,  1.0000],\n",
       "        [-0.4161,  0.9957,  1.0000],\n",
       "        [-0.9900,  0.9903,  1.0000],\n",
       "        [-0.6536,  0.9828,  1.0000],\n",
       "        [ 0.2837,  0.9732,  0.9999],\n",
       "        [ 0.9602,  0.9615,  0.9999],\n",
       "        [ 0.7539,  0.9477,  0.9999],\n",
       "        [-0.1455,  0.9318,  0.9999],\n",
       "        [-0.9111,  0.9140,  0.9998]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(odd_PE.shape)\n",
    "odd_PE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do next is interleave the matrices, `even_PE` and `odd_PE` above. What this means is that the first index in the first vector of the `even_PE` matrix should be position 0 and the first index in the first vector of the `odd_PE` matrix should be position 1, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  1.0000],\n",
       "         [ 0.0000,  1.0000],\n",
       "         [ 0.0000,  1.0000]],\n",
       "\n",
       "        [[ 0.8415,  0.5403],\n",
       "         [ 0.0464,  0.9989],\n",
       "         [ 0.0022,  1.0000]],\n",
       "\n",
       "        [[ 0.9093, -0.4161],\n",
       "         [ 0.0927,  0.9957],\n",
       "         [ 0.0043,  1.0000]],\n",
       "\n",
       "        [[ 0.1411, -0.9900],\n",
       "         [ 0.1388,  0.9903],\n",
       "         [ 0.0065,  1.0000]],\n",
       "\n",
       "        [[-0.7568, -0.6536],\n",
       "         [ 0.1846,  0.9828],\n",
       "         [ 0.0086,  1.0000]],\n",
       "\n",
       "        [[-0.9589,  0.2837],\n",
       "         [ 0.2300,  0.9732],\n",
       "         [ 0.0108,  0.9999]],\n",
       "\n",
       "        [[-0.2794,  0.9602],\n",
       "         [ 0.2749,  0.9615],\n",
       "         [ 0.0129,  0.9999]],\n",
       "\n",
       "        [[ 0.6570,  0.7539],\n",
       "         [ 0.3192,  0.9477],\n",
       "         [ 0.0151,  0.9999]],\n",
       "\n",
       "        [[ 0.9894, -0.1455],\n",
       "         [ 0.3629,  0.9318],\n",
       "         [ 0.0172,  0.9999]],\n",
       "\n",
       "        [[ 0.4121, -0.9111],\n",
       "         [ 0.4057,  0.9140],\n",
       "         [ 0.0194,  0.9998]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked = torch.stack(\n",
    "    [even_PE, odd_PE], dim=2\n",
    ")  # This method also creates a new dimension\n",
    "\n",
    "print(stacked.shape)\n",
    "stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
       "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
       "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
       "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
       "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
       "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
       "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "\n",
    "print(PE.shape)\n",
    "PE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By flattening the `stacked` matrix, we are going to effectively be getting the interleavement here.\n",
    "\n",
    "For the first vector in the `PE` matrix, we will have the positional encoding for the first word. For the second word, the positional encoding is the second vector, and so on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_sequence_length, d_model):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        print(f\"even.shape: {even_i.shape}\")\n",
    "\n",
    "        denominator = torch.pow(10000, even_i / self.d_model)\n",
    "        print(f\"denominator.shape: {denominator.shape}\")\n",
    "\n",
    "        # or .reshape(self.max_sequence_length, 1)\n",
    "        position = torch.arange(self.max_sequence_length, dtype=float).reshape(-1, 1)\n",
    "        print(f\"position.shape: {position.shape}\")\n",
    "\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        print(f\"even_PE.shape: {even_PE.shape}\")\n",
    "\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        print(f\"odd_PE.shape: {odd_PE.shape}\")\n",
    "\n",
    "        # This method stacks the two matrices in a new dimension\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        print(f\"stacked.shape: {stacked.shape}\")\n",
    "\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        print(f\"PE.shape: {PE.shape}\")\n",
    "\n",
    "        return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "even.shape: torch.Size([3])\n",
      "denominator.shape: torch.Size([3])\n",
      "position.shape: torch.Size([10, 1])\n",
      "even_PE.shape: torch.Size([10, 3])\n",
      "odd_PE.shape: torch.Size([10, 3])\n",
      "stacked.shape: torch.Size([10, 3, 2])\n",
      "PE.shape: torch.Size([10, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0464,  0.9989,  0.0022,  1.0000],\n",
       "        [ 0.9093, -0.4161,  0.0927,  0.9957,  0.0043,  1.0000],\n",
       "        [ 0.1411, -0.9900,  0.1388,  0.9903,  0.0065,  1.0000],\n",
       "        [-0.7568, -0.6536,  0.1846,  0.9828,  0.0086,  1.0000],\n",
       "        [-0.9589,  0.2837,  0.2300,  0.9732,  0.0108,  0.9999],\n",
       "        [-0.2794,  0.9602,  0.2749,  0.9615,  0.0129,  0.9999],\n",
       "        [ 0.6570,  0.7539,  0.3192,  0.9477,  0.0151,  0.9999],\n",
       "        [ 0.9894, -0.1455,  0.3629,  0.9318,  0.0172,  0.9999],\n",
       "        [ 0.4121, -0.9111,  0.4057,  0.9140,  0.0194,  0.9998]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(max_sequence_length=10, d_model=6)\n",
    "PE = pe.forward()\n",
    "\n",
    "PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
