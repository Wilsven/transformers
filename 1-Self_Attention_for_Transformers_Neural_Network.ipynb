{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Attention in Transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer-architecture-1.png\" width=\"500\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`L` is going to be the length of the input sequence. As an example, assume the input sequence: *My name is John*. The sequence has 4 words. Hence, `L` in this case would be 4.\n",
    "\n",
    "For illustrative purposes, `d_k` and `d_v` are going to be the sizes of the matrices, `q`, `k` and `v` which are both equal to 8.\n",
    "\n",
    "The vectors are then initialised via the normal distribution using the `np.random.randn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, d_k, d_v = 4, 8, 8\n",
    "\n",
    "# Generate Query, Key and Value vectors\n",
    "q = np.random.randn(L, d_k)\n",
    "k = np.random.randn(L, d_k)\n",
    "v = np.random.randn(L, d_v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every single word, it will be represented as an 8 x 1 vector in the matrices, `q`, `k` and `v` as shown below.\n",
    "\n",
    "For example, `q[0]`, `k[0]` and `v[0]` will all represent the word *My* in the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.34972467  1.35332753 -0.77556227  1.91676259 -1.61592118  0.59815915\n",
      "  -0.18490108  1.15882701]\n",
      " [-0.33608825 -1.19344485 -0.97662111  1.20519669  0.2540626   0.45435371\n",
      "  -2.21570849 -0.55022568]\n",
      " [ 1.33711846  0.01455579 -0.61734755  0.34603767 -1.30180037  0.92430392\n",
      "  -0.80426423 -0.1787877 ]\n",
      " [-0.44801764  0.64280112  0.40218577 -0.28686598  0.38526026  0.6576787\n",
      "  -0.08885837 -0.07665715]]\n",
      "K\n",
      " [[-0.96890619  0.96396798 -0.73914589  0.12728142 -0.134454    0.2575052\n",
      "  -0.25883817 -0.97963846]\n",
      " [ 1.42722162  0.36437203 -0.33366425 -1.58075897 -0.18005312  0.81230955\n",
      "   2.22149487 -0.94734918]\n",
      " [ 0.19142128 -1.33368536 -1.07531501 -0.2893011   0.30201615  0.06685315\n",
      "  -0.07141276 -1.27233625]\n",
      " [ 0.41115131 -1.01680208  1.93874384 -1.34373299 -0.60134847  0.26848415\n",
      "   0.01355881 -1.11879544]]\n",
      "V\n",
      " [[-0.82589176 -0.8639556   0.75185056  1.36580367 -0.80202648  0.53314709\n",
      "   0.60224679 -0.13482091]\n",
      " [-1.21390252  0.20359992  0.26225157  0.03961611 -0.15492762  0.64232898\n",
      "  -1.17509491 -1.49053203]\n",
      " [-0.58725654  0.42888178 -1.03420484  0.65297388 -0.45357274  1.74744249\n",
      "  -1.14826684  0.23966213]\n",
      " [ 0.46585498  0.09688181  0.20670099 -1.17548068  0.9505946  -1.85844719\n",
      "   0.89936342 -1.7963295 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention\n",
    "\n",
    "In order to create the initial self attention matrix, we need every word to look at every other word to determine if there was a high affinity towards another word or not.\n",
    "\n",
    "This is represented by the query, $Q$ which is, for every word, what is it that I am looking for and the key, $K$ which is, what I currently have."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\n",
    "\\text{self attention} = \\text{softmax}(\\frac{Q·K^{T}}{\\sqrt{d_{k}}} + M) \\nonumber\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "\\text{new } V = \\text{self attention}·V \\nonumber\n",
    " \n",
    "\\end{align}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix multiplication between $Q$ and $K$ leads to a 4 x 4 matrix because our input sequence is of length 4. The resulting matrix holds values which is proportional to exactly how much attention we want to focus on for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09795386, -1.08341991, -3.17635367, -5.0670283 ],\n",
       "       [ 1.24583087, -6.57139034,  3.19426403, -1.88281241],\n",
       "       [ 0.01520996,  0.94057239,  0.75381478,  0.09321527],\n",
       "       [ 0.93559223,  0.2541633 , -1.0283383 ,  0.3568614 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matmul(q, k.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we need the denominator, $\\sqrt{d_{k}}$ ?\n",
    "\n",
    "This is because we want to minimise the variance and stabilise the values of the $Q·K^{T}$ matrix.\n",
    "\n",
    "We can view the variance of the `q` and `k` matrices as well as the resulting matrix from the matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8941885476429156, 0.8540676867127687, 5.722632018415711)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Why do we need sqrt(d_k) in denominator?\n",
    "q.var(), k.var(), np.matmul(q, k.T).var()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that while the variance of the `q` and `k` matrices are close to 1, the resulting matrix have a much higher variance. Therefore, in order to make sure we stabilise these values and reduce the variance, we divide the resulting matrix by the square root of the dimension of the query matrix, $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8941885476429156, 0.8540676867127687, 0.7153290023019637)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "\n",
    "q.var(), k.var(), scaled.var()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can see that the variance are more or less in the same range (i.e. close to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03463192, -0.38304678, -1.12301061, -1.79146503],\n",
       "       [ 0.44046773, -2.32333734,  1.12934288, -0.66567471],\n",
       "       [ 0.00537753,  0.33254256,  0.26651377,  0.03295658],\n",
       "       [ 0.3307818 ,  0.0898603 , -0.36357249,  0.12616956]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "Masking is not required in the encoders, but required in the decoders. \n",
    "\n",
    "It is required in the decoders to ensure that words don't get context from words generated in the future. That would be considered cheating and in reality, you don't know the word that is going to be generated next. Therefore, it does not make sense to generate your vectors based off of those future words.\n",
    "\n",
    "However, as for the encoder, masking isn't required because all of our inputs from the input sequence gets passed into the encoder simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 1., 0., 0.],\n",
       "       [1., 1., 1., 0.],\n",
       "       [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.tril(np.ones((L, L)))\n",
    "mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The triangular matrix above will simulate the aforementioned masking. For example, recall our input sequence: *My name is John*.\n",
    "\n",
    "The word *My* wouldn't be able to get context from the words after it. This is represented by the first row of the mask matrix above where all other words are 0 except for itself. \n",
    "\n",
    "Similarly, for the word *is*, it has contextual information of previous words *My* and *name* (and itself, of course) while having no context from *John*, which comes after it. Therefore, the third row in the mask matrix above has 1's for the words *My*, *name* and *is* and 0 for *John*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -inf, -inf, -inf],\n",
       "       [  0.,   0., -inf, -inf],\n",
       "       [  0.,   0.,   0., -inf],\n",
       "       [  0.,   0.,   0.,   0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask == 0] = -np.infty\n",
    "mask[mask == 1] = 0\n",
    "mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1 values in the mask is converted into 0 because when we add the mask to the `scaled` matrix, the bottom diagonal in matrix remains the same (because it is simply an addition with 0).\n",
    "\n",
    "The 0 values in the mask is converted into negative infinity because of the softmax function which we will be applying to the `scaled` matrix. Softmax of negative infinity results in 0 (which is what we ideally want from the mask)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03463192,        -inf,        -inf,        -inf],\n",
       "       [ 0.44046773, -2.32333734,        -inf,        -inf],\n",
       "       [ 0.00537753,  0.33254256,  0.26651377,        -inf],\n",
       "       [ 0.3307818 ,  0.0898603 , -0.36357249,  0.12616956]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled + mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "The softmax operation is used to convert a vector into a probability distribution. Therefore, their values will add up to 1 and it is very interpretable and stable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\n",
    "\\text{softmax} = \\frac{e^{x_{i}}}{\\sum_{j}e^{x}_{j}} \\nonumber\n",
    "\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        , 0.        , 0.        ],\n",
       "       [0.94068829, 0.05931171, 0.        , 0.        ],\n",
       "       [0.2713384 , 0.37635459, 0.35230701, 0.        ],\n",
       "       [0.32255324, 0.25349566, 0.16108206, 0.26286904]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = softmax(scaled + mask)\n",
    "attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform a matrix multiplication between the attention matrix and `v` matrix. The resulting matrix should better encapsulate the context of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82589176, -0.8639556 ,  0.75185056,  1.36580367, -0.80202648,\n",
       "         0.53314709,  0.60224679, -0.13482091],\n",
       "       [-0.84890534, -0.80063705,  0.72281161,  1.28714522, -0.76364594,\n",
       "         0.53962286,  0.49682961, -0.21523046],\n",
       "       [-0.88784853, -0.0067005 , -0.06165211,  0.61555196, -0.43572516,\n",
       "         1.00204298, -0.68338215, -0.51311602],\n",
       "       [-0.54625073, -0.13250759,  0.19673492,  0.24677181, -0.12115026,\n",
       "         0.12774933, -0.05217519, -0.85492447]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_v = np.matmul(attention, v)\n",
    "new_v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the new `v` matrix to the previous `v` matrix, we can see that the first row, which corresponds to the first word, *My* is very similar. \n",
    "\n",
    "This is a direct effect of the matrix multiplication with the masked matrix where the words in the first row after the first word are masked. Whereas, when you move down the rows of the matrix to the later words, you will notice how different the vectors are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.82589176, -0.8639556 ,  0.75185056,  1.36580367, -0.80202648,\n",
       "         0.53314709,  0.60224679, -0.13482091],\n",
       "       [-1.21390252,  0.20359992,  0.26225157,  0.03961611, -0.15492762,\n",
       "         0.64232898, -1.17509491, -1.49053203],\n",
       "       [-0.58725654,  0.42888178, -1.03420484,  0.65297388, -0.45357274,\n",
       "         1.74744249, -1.14826684,  0.23966213],\n",
       "       [ 0.46585498,  0.09688181,  0.20670099, -1.17548068,  0.9505946 ,\n",
       "        -1.85844719,  0.89936342, -1.7963295 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return (np.exp(x).T / np.sum(np.exp(x), axis=-1)).T\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = k.shape[-1]\n",
    "    scaled = np.matmul(q, k.T) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = softmax(scaled)\n",
    "    out = np.matmul(attention, v)\n",
    "    return out, attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `mask` parameter is optional as the function can be used in the encoder as well as in the decoder. As mentioned before, the encoder does not require a mask as the inputs from the input sequence are passed into the encoder simultaneously."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.34972467  1.35332753 -0.77556227  1.91676259 -1.61592118  0.59815915\n",
      "  -0.18490108  1.15882701]\n",
      " [-0.33608825 -1.19344485 -0.97662111  1.20519669  0.2540626   0.45435371\n",
      "  -2.21570849 -0.55022568]\n",
      " [ 1.33711846  0.01455579 -0.61734755  0.34603767 -1.30180037  0.92430392\n",
      "  -0.80426423 -0.1787877 ]\n",
      " [-0.44801764  0.64280112  0.40218577 -0.28686598  0.38526026  0.6576787\n",
      "  -0.08885837 -0.07665715]]\n",
      "K\n",
      " [[-0.96890619  0.96396798 -0.73914589  0.12728142 -0.134454    0.2575052\n",
      "  -0.25883817 -0.97963846]\n",
      " [ 1.42722162  0.36437203 -0.33366425 -1.58075897 -0.18005312  0.81230955\n",
      "   2.22149487 -0.94734918]\n",
      " [ 0.19142128 -1.33368536 -1.07531501 -0.2893011   0.30201615  0.06685315\n",
      "  -0.07141276 -1.27233625]\n",
      " [ 0.41115131 -1.01680208  1.93874384 -1.34373299 -0.60134847  0.26848415\n",
      "   0.01355881 -1.11879544]]\n",
      "V\n",
      " [[-0.82589176 -0.8639556   0.75185056  1.36580367 -0.80202648  0.53314709\n",
      "   0.60224679 -0.13482091]\n",
      " [-1.21390252  0.20359992  0.26225157  0.03961611 -0.15492762  0.64232898\n",
      "  -1.17509491 -1.49053203]\n",
      " [-0.58725654  0.42888178 -1.03420484  0.65297388 -0.45357274  1.74744249\n",
      "  -1.14826684  0.23966213]\n",
      " [ 0.46585498  0.09688181  0.20670099 -1.17548068  0.9505946  -1.85844719\n",
      "   0.89936342 -1.7963295 ]]\n",
      "New V\n",
      " [[-0.81301557 -0.27157708  0.29658996  0.6597375  -0.41872737  0.56516595\n",
      "  -0.18165415 -0.62348638]\n",
      " [-0.56650317  0.01035055 -0.3612093   0.67342789 -0.41371854  1.01578563\n",
      "  -0.43157899 -0.10214713]\n",
      " [-0.59261418  0.01589016 -0.00312596  0.22493883 -0.1333774   0.37818778\n",
      "  -0.33819514 -0.79297693]\n",
      " [-0.54625073 -0.13250759  0.19673492  0.24677181 -0.12115026  0.12774933\n",
      "  -0.05217519 -0.85492447]]\n",
      "Attention\n",
      " [[0.46863853 0.30863305 0.14725845 0.07546997]\n",
      " [0.29538971 0.01862473 0.58826098 0.09772458]\n",
      " [0.21216117 0.29427399 0.27547103 0.21809381]\n",
      " [0.32255324 0.25349566 0.16108206 0.26286904]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q, k, v)\n",
    "\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 1.34972467  1.35332753 -0.77556227  1.91676259 -1.61592118  0.59815915\n",
      "  -0.18490108  1.15882701]\n",
      " [-0.33608825 -1.19344485 -0.97662111  1.20519669  0.2540626   0.45435371\n",
      "  -2.21570849 -0.55022568]\n",
      " [ 1.33711846  0.01455579 -0.61734755  0.34603767 -1.30180037  0.92430392\n",
      "  -0.80426423 -0.1787877 ]\n",
      " [-0.44801764  0.64280112  0.40218577 -0.28686598  0.38526026  0.6576787\n",
      "  -0.08885837 -0.07665715]]\n",
      "K\n",
      " [[-0.96890619  0.96396798 -0.73914589  0.12728142 -0.134454    0.2575052\n",
      "  -0.25883817 -0.97963846]\n",
      " [ 1.42722162  0.36437203 -0.33366425 -1.58075897 -0.18005312  0.81230955\n",
      "   2.22149487 -0.94734918]\n",
      " [ 0.19142128 -1.33368536 -1.07531501 -0.2893011   0.30201615  0.06685315\n",
      "  -0.07141276 -1.27233625]\n",
      " [ 0.41115131 -1.01680208  1.93874384 -1.34373299 -0.60134847  0.26848415\n",
      "   0.01355881 -1.11879544]]\n",
      "V\n",
      " [[-0.82589176 -0.8639556   0.75185056  1.36580367 -0.80202648  0.53314709\n",
      "   0.60224679 -0.13482091]\n",
      " [-1.21390252  0.20359992  0.26225157  0.03961611 -0.15492762  0.64232898\n",
      "  -1.17509491 -1.49053203]\n",
      " [-0.58725654  0.42888178 -1.03420484  0.65297388 -0.45357274  1.74744249\n",
      "  -1.14826684  0.23966213]\n",
      " [ 0.46585498  0.09688181  0.20670099 -1.17548068  0.9505946  -1.85844719\n",
      "   0.89936342 -1.7963295 ]]\n",
      "New V\n",
      " [[-0.82589176 -0.8639556   0.75185056  1.36580367 -0.80202648  0.53314709\n",
      "   0.60224679 -0.13482091]\n",
      " [-0.84890534 -0.80063705  0.72281161  1.28714522 -0.76364594  0.53962286\n",
      "   0.49682961 -0.21523046]\n",
      " [-0.88784853 -0.0067005  -0.06165211  0.61555196 -0.43572516  1.00204298\n",
      "  -0.68338215 -0.51311602]\n",
      " [-0.54625073 -0.13250759  0.19673492  0.24677181 -0.12115026  0.12774933\n",
      "  -0.05217519 -0.85492447]]\n",
      "Attention\n",
      " [[1.         0.         0.         0.        ]\n",
      " [0.94068829 0.05931171 0.         0.        ]\n",
      " [0.2713384  0.37635459 0.35230701 0.        ]\n",
      " [0.32255324 0.25349566 0.16108206 0.26286904]]\n"
     ]
    }
   ],
   "source": [
    "values, attention = scaled_dot_product_attention(q, k, v, mask=mask)\n",
    "\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"New V\\n\", values)\n",
    "print(\"Attention\\n\", attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
