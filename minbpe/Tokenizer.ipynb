{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer ):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "* [Introduction](#intro)\n",
    "    * [Unicode Code Points](#code-points)\n",
    "    * [UTF-8](#utf-8)\n",
    "    * [Bytes](#bytes)\n",
    "    * [Other encoding types](#other-encodings)\n",
    "* [Byte Pair Encoding](#bpe)\n",
    "    * [`get_stats`](#get_stats)\n",
    "    * [`merge`](#merge)\n",
    "* [Putting it all together](#together)\n",
    "* [Tokenizer](#tokenizer)\n",
    "    * [Decoding](#decoding)\n",
    "    * [Encoding](#encoding)\n",
    "        * [Fancy code explanation](#fancy)\n",
    "* [Forced splits using regex patterns (GPT Series)](#forced_splits)\n",
    "    * [How do numbers get captured by the regex pattern?](#numbers)\n",
    "    * [How do apostrophes get captured by the regex pattern?](#apostrophes)\n",
    "    * [How do punctuations get captured by the regex pattern?](#punctuations)\n",
    "    * [How do unecessary whitespaces get captured by the regex pattern?](#whitespaces)\n",
    "    * [Example: Python code](#example-python)\n",
    "* [Tiktoken](#tiktoken)\n",
    "* [Special Tokens](#special-tokens)\n",
    "* [Exercise](#exercise)\n",
    "* [sentencepiece](#sentencepiece)\n",
    "    * [`byte_fallback`](#byte-fallback)\n",
    "    * [`add_dummy_prefix`](#add-dummy-prefix)\n",
    "    * [Llama 2 tokenizer proto](#llama-2-tokenizer-proto)\n",
    "    * [`vocab_size`](#vocab-size)\n",
    "* [Extension of `vocab_size`](#extension)\n",
    "* [Learning to Compress Prompts with Gist Tokens](#gist-tokens)\n",
    "* [Multimodality](#multimodality)\n",
    "* [Revisiting and explaining the quirks of LLM tokenization](#revisiting-and-explaining-the-quirks-of-llm-tokenization)\n",
    "    * [Why can't LLM spell words?](#why-cant-llm-spell-words)\n",
    "    * [Why is LLM worse at non-English languages (e.g. Japanese)?](#why-is-llm-worse-at-non-english-languages)\n",
    "    * [Why is LLM bad at simple arithmetic?](#why-is-llm-bad-at-simple-arithmetic)\n",
    "    * [Why did GPT-2 have more than necessary trouble coding in Python?](#why-did-gpt-2-have-more-than-necessary-trouble-coding-in-python)\n",
    "    * [Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"?](#why-did-my-llm-abruptly-halt-when-it-sees-the-string-end-of-text)\n",
    "    * [What is this weird warning I get about a \"trailing whitespace\"?](#what-is-this-weird-warning-i-get-about-a-trailing-whitespace)\n",
    "    * [Why the LLM break if I ask it about \"SolidGoldMagikarp\"?](#why-the-llm-break-if-i-ask-it-about-solidgoldmagikarp)\n",
    "    * [Why should I prefer to use YAML over JSON with LLMs?](#why-should-i-prefer-to-use-yaml-over-json-with-llms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "- [Why can't LLM spell words?](why-cant-llm-spell-words) **Tokenization**.\n",
    "- Why can't LLM do super simple string processing tasks like reversing a string? **Tokenization**.\n",
    "- [Why is LLM worse at non-English languages (e.g. Japanese)?](#why-is-llm-worse-at-non-english-languages) **Tokenization**.\n",
    "- [Why is LLM bad at simple arithmetic?](#why-is-llm-bad-at-simple-arithmetic) **Tokenization**.\n",
    "- [Why did GPT-2 have more than necessary trouble coding in Python?](#why-did-gpt-2-have-more-than-necessary-trouble-coding-in-python) **Tokenization**.\n",
    "- [Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"?](#why-did-my-llm-abruptly-halt-when-it-sees-the-string-end-of-text) **Tokenization**.\n",
    "- [What is this weird warning I get about a \"trailing whitespace\"?](#what-is-this-weird-warning-i-get-about-a-trailing-whitespace) **Tokenization**.\n",
    "- [Why the LLM break if I ask it about \"SolidGoldMagikarp\"?](#why-the-llm-break-if-i-ask-it-about-solidgoldmagikarp) **Tokenization**.\n",
    "- [Why should I prefer to use YAML over JSON with LLMs?](#why-should-i-prefer-to-use-yaml-over-json-with-llms) **Tokenization**.\n",
    "- Why is LLM not actually end-to-end language modeling? **Tokenization**.\n",
    "- What is the real root of suffering? **Tokenization**.\n",
    "\n",
    "After we go through this notebook, at the end, we will [revisit these questions](#revisiting-and-explaining-the-quirks-of-llm-tokenization) again.\n",
    " \n",
    "---\n",
    "\n",
    "Good tokenization web app: https://tiktokenizer.vercel.app\n",
    "\n",
    "Example string:\n",
    "\n",
    "```\n",
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "127 + 677 = 804\n",
    "1275 + 6773 = 8041\n",
    "\n",
    "Egg.\n",
    "I have an Egg.\n",
    "egg.\n",
    "EGG.\n",
    "\n",
    "만나서 반가워요. 저는 OpenAI에서 개발한 대규모 언어 모델인 ChatGPT입니다. 궁금한 것이 있으시면 무엇이든 물어보세요.\n",
    "\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Much glory awaits someone who can delete the need for tokenization. But meanwhile, let's learn about it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id=\"intro\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings are immutable sequences of Unicode code points. See [here](https://docs.python.org/3/library/stdtypes.html#:~:text=Strings%20are%20immutable%20sequences%20of%20Unicode%20code%20points.) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"안녕하세요 👋 (hello in Korean!)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unicode Code Points <a id=\"code-points\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the Unicode code point for each character. The Unicode code points are numbers assigned by the Unicode Consortium to every character in every writing system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `ord` function only takes in a character\n",
    "print([ord(x) for x in \"안녕하세요 👋 (hello in Korean!)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTF-8 <a id=\"utf-8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw code points are already integers, so why can't we simply use these integers and not have any tokenization at all? One reason for that is that the vocabulary would be quite long. For Unicodes, there will be a vocabulary size of about [150,000 different code points](https://en.wikipedia.org/wiki/Unicode#:~:text=%5BA%5D%20defines-,149813%20characters,-%5B3%5D%20and). However, more worryingly than that is the Unicode standard is very much alive and keeps changing. Therefore, it's not necessarily a stable representation that we may want to use directly.\n",
    "\n",
    "For those reasons, we need something better and we turn to encodings. The Unicode Consortium defines [three types of encodings](https://en.wikipedia.org/wiki/Unicode#:~:text=The%20Unicode%20Standard%20itself%20defines%20three%20encodings%3A%20UTF%2D8%2C%20UTF%2D16%2C%20and%20UTF%2D32): UTF-8, UTF-16, and UTF-32. We will use UTF-8 for this tutorial. **These encodings are the ways which we can take Unicode text and translate into binary data or byte streams.** UTF-8 is by far the most common encoding.\n",
    "\n",
    "UTF-8 takes every single Unicode code point and translates it into byte streams and these are between 1 and 4 bytes.\n",
    "Let's encode the string above into UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode string into UTF-8\n",
    "print(list(\"안녕하세요 👋 (hello in Korean!)\".encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bytes <a id=\"bytes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from above contains integers which are the representation of their corresponding bytes. Let's see how we can view the output above as a byte stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the raw bytes\n",
    "print(\n",
    "    \"안녕하세요 👋 (hello in Korean!)\".encode(\"utf-8\")\n",
    ")  # you can use the `bytes` function too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other encoding types <a id=\"other-encodings\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even encode the text in UTF-16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode string into UTF-16\n",
    "print(list(\"안녕하세요 👋 (hello in Korean!)\".encode(\"utf-16\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see a disadvantage of UTF-16. We see that there are many instances where there is a zero followed another integer. Therefore, we can get a sense that this is a bit of a wasteful encoding. You can run the same code above with UTF-32 and you will get a sense of the wastefulness of encoding with UTF-16 or UTF-32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with UTF-32 encoding here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suffice to say, we would like to stick with UTF-8 encoding. However, if we use UTF-8 naively (which are byte streams), this would imply a vocabulary length of only 256 possible tokens (1 byte = 8 bits = $2^{8}$ = 256) This vocabulary size is very very small. What this is going to do if we were to use this naively is that all our text would be stretched out over very very long sequences of bytes. What this does is the embedding table is going to be very tiny (as well as the prediction at the top, the final layer is also going to be tiny) but our sequences are very long. Additionally, we have a finite context length (for computational reasons) in the attention that can be supported in the transformer architecture. \n",
    "\n",
    "Finite context lengths and very long sequences is inefficient and is not going to allow the attention to attend to tokens before in sufficiently long text for the purpose of next token prediction task. Therefore, we don't want to use the raw bytes of UTF-8 encoding. We want to support larger vocabulary size that we can tune as a hyperparameter and also stick to the UTF-8 encoding. What do we do? What\n",
    "\n",
    "Byte pair encoding!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding <a id=\"bpe\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The byte pair encoding (BPE) algorithm allows us to compress these byte sequences to a variable amount (which we'll get to this in a bit). The BPE algorithm is not all that complicated and the [Wikepedia page](https://en.wikipedia.org/wiki/Byte_pair_encoding) is actually quite instructive as far as the basic idea goes.\n",
    "\n",
    "Let's code it out with some sample text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text from https://www.reedbeta.com/blog/programmers-intro-to-Unicode/\n",
    "text = \"Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺‌🇳‌🇮‌🇨‌🇴‌🇩‌🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.\"\n",
    "\n",
    "tokens = text.encode(\"utf-8\")  # raw bytes\n",
    "tokens = list(\n",
    "    map(int, tokens)\n",
    ")  # convert to a list of integers in range 0..255 for convenience\n",
    "\n",
    "print(\"---\")\n",
    "print(text)\n",
    "print(\"length:\", len(text))\n",
    "print(\"---\")\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the length of the raw bytes is more than the length of the text. This is because for simple words, the UTF-8 encoding result in a single integer. But for complex characters (like in the beginning of the text) can result in multiple bytes (i.e. two to four bytes).\n",
    "\n",
    "Now, for the first step of the algorithm, what we'd like to do is to find the most commonly occurring pair of bytes. Then, we're going to merge it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `get_stats` <a id=\"get_stats\"></a>\n",
    "\n",
    "This function iterates through the sequence and counts the occurences of each pair. Results are stored in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):  # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "stats = get_stats(tokens)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reformat the dictionary into a list of tuples where the first item in the tuples represent the counts and the second item in the tuples represent the pairs. This list of tuples are also sorted such that the most common pair is first in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(((v, k) for k, v in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like (100, 32) is the most commonly occurring pair and it occurred 20 times. To see what this pair represents, we can decode it using `chr`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means in our original text, there were a lot of e's followed by spaces.\n",
    "\n",
    "Now that we've identified the most common pair, we would like to iterate over the sequence and mint a new token for every one of the token pair (the most common pair) above. This new token will have the new token ID of 256 because the sequence has IDs going from 0 to 255 (256 integers).\n",
    "\n",
    "Using the (101, 32) pair as an example, whenever we see a (101, 32) pair while iterating through the sequence, we will \"swap it out\" (or merge it) with a new token, 256. Let's implement this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `merge` <a id=\"merge\"></a>\n",
    "\n",
    "This function merges the most common pair into a newly minted token. We will store these merge rules in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most common pair from the `stats` dictionary\n",
    "top_pair = max(stats, key=stats.get)\n",
    "print(top_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    # In the list of ints (ids), replace all consecutive\n",
    "    # occurences of pair with the new token id\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # If we are not at the very last position AND the pair matches,\n",
    "        # replace it with the new token id\n",
    "        if (i < len(ids) - 1) and (ids[i] == pair[0] and ids[i + 1] == pair[1]):\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "\n",
    "    return newids\n",
    "\n",
    "\n",
    "# Example\n",
    "print(merge([5, 6, 6, 7, 9, 1], (6, 7), 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "\n",
    "print(\"length:\", len(tokens2))\n",
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we had a token length of 616. After the first iteration of the BPE algorithm, we now have a token length of 596. This makes sense as we had 20 counts for our `top_pair`,  (101, 32) which has now been replaced with the new token 256. \n",
    "\n",
    "**Tip:** `Ctrl + F` to search for token \"256\" and you will see 20 of them. Do the same and search for tokens \"101, 32\" and you will not find any consecutive pairs. Do note that tokens 101 or 32 can still occur separately.\n",
    "\n",
    "Now, we can write a function to iterate this! The high level steps are as follow:\n",
    "\n",
    "1. Go over the sequence\n",
    "2. Find the most commmon pair\n",
    "3. Merge it\n",
    "4. Repeat (hyperparameter)\n",
    "\n",
    "How many times do we iterate for? It is totally up to us as a hyperparameter. The more merges we do, the larger our vocabulary size will become and the shorter our sequence length will be. There is some sweet spot that we will usually find that works the best in practice. As an example, GPT-4 currently uses a vocabulary size of roughly 100,000 tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together <a id=\"together\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the training text longer to have more representative token statistics\n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-Unicode/\n",
    "text = \"\"\"A Programmer’s Introduction to Unicode March 3, 2017 · Coding · 22 Comments  Ｕｎｉｃｏｄｅ! 🅤🅝🅘🅒🅞🅓🅔‽ 🇺\\u200c🇳\\u200c🇮\\u200c🇨\\u200c🇴\\u200c🇩\\u200c🇪! 😄 The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to “support Unicode” in our software (whatever that means—like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I don’t blame programmers for still finding the whole thing mysterious, even 30 years after Unicode’s inception.  A few months ago, I got interested in Unicode and decided to spend some time learning more about it in detail. In this article, I’ll give an introduction to it from a programmer’s point of view.  I’m going to focus on the character set and what’s involved in working with strings and files of Unicode text. However, in this article I’m not going to talk about fonts, text layout/shaping/rendering, or localization in detail—those are separate issues, beyond my scope (and knowledge) here.  Diversity and Inherent Complexity The Unicode Codespace Codespace Allocation Scripts Usage Frequency Encodings UTF-8 UTF-16 Combining Marks Canonical Equivalence Normalization Forms Grapheme Clusters And More… Diversity and Inherent Complexity As soon as you start to study Unicode, it becomes clear that it represents a large jump in complexity over character sets like ASCII that you may be more familiar with. It’s not just that Unicode contains a much larger number of characters, although that’s part of it. Unicode also has a great deal of internal structure, features, and special cases, making it much more than what one might expect a mere “character set” to be. We’ll see some of that later in this article.  When confronting all this complexity, especially as an engineer, it’s hard not to find oneself asking, “Why do we need all this? Is this really necessary? Couldn’t it be simplified?”  However, Unicode aims to faithfully represent the entire world’s writing systems. The Unicode Consortium’s stated goal is “enabling people around the world to use computers in any language”. And as you might imagine, the diversity of written languages is immense! To date, Unicode supports 135 different scripts, covering some 1100 languages, and there’s still a long tail of over 100 unsupported scripts, both modern and historical, which people are still working to add.  Given this enormous diversity, it’s inevitable that representing it is a complicated project. Unicode embraces that diversity, and accepts the complexity inherent in its mission to include all human writing systems. It doesn’t make a lot of trade-offs in the name of simplification, and it makes exceptions to its own rules where necessary to further its mission.  Moreover, Unicode is committed not just to supporting texts in any single language, but also to letting multiple languages coexist within one text—which introduces even more complexity.  Most programming languages have libraries available to handle the gory low-level details of text manipulation, but as a programmer, you’ll still need to know about certain Unicode features in order to know when and how to apply them. It may take some time to wrap your head around it all, but don’t be discouraged—think about the billions of people for whom your software will be more accessible through supporting text in their language. Embrace the complexity!  The Unicode Codespace Let’s start with some general orientation. The basic elements of Unicode—its “characters”, although that term isn’t quite right—are called code points. Code points are identified by number, customarily written in hexadecimal with the prefix “U+”, such as U+0041 “A” latin capital letter a or U+03B8 “θ” greek small letter theta. Each code point also has a short name, and quite a few other properties, specified in the Unicode Character Database.  The set of all possible code points is called the codespace. The Unicode codespace consists of 1,114,112 code points. However, only 128,237 of them—about 12% of the codespace—are actually assigned, to date. There’s plenty of room for growth! Unicode also reserves an additional 137,468 code points as “private use” areas, which have no standardized meaning and are available for individual applications to define for their own purposes.  Codespace Allocation To get a feel for how the codespace is laid out, it’s helpful to visualize it. Below is a map of the entire codespace, with one pixel per code point. It’s arranged in tiles for visual coherence; each small square is 16×16 = 256 code points, and each large square is a “plane” of 65,536 code points. There are 17 planes altogether.  Map of the Unicode codespace (click to zoom)  White represents unassigned space. Blue is assigned code points, green is private-use areas, and the small red area is surrogates (more about those later). As you can see, the assigned code points are distributed somewhat sparsely, but concentrated in the first three planes.  Plane 0 is also known as the “Basic Multilingual Plane”, or BMP. The BMP contains essentially all the characters needed for modern text in any script, including Latin, Cyrillic, Greek, Han (Chinese), Japanese, Korean, Arabic, Hebrew, Devanagari (Indian), and many more.  (In the past, the codespace was just the BMP and no more—Unicode was originally conceived as a straightforward 16-bit encoding, with only 65,536 code points. It was expanded to its current size in 1996. However, the vast majority of code points in modern text belong to the BMP.)  Plane 1 contains historical scripts, such as Sumerian cuneiform and Egyptian hieroglyphs, as well as emoji and various other symbols. Plane 2 contains a large block of less-common and historical Han characters. The remaining planes are empty, except for a small number of rarely-used formatting characters in Plane 14; planes 15–16 are reserved entirely for private use.  Scripts Let’s zoom in on the first three planes, since that’s where the action is:  Map of scripts in Unicode planes 0–2 (click to zoom)  This map color-codes the 135 different scripts in Unicode. You can see how Han () and Korean () take up most of the range of the BMP (the left large square). By contrast, all of the European, Middle Eastern, and South Asian scripts fit into the first row of the BMP in this diagram.  Many areas of the codespace are adapted or copied from earlier encodings. For example, the first 128 code points of Unicode are just a copy of ASCII. This has clear benefits for compatibility—it’s easy to losslessly convert texts from smaller encodings into Unicode (and the other direction too, as long as no characters outside the smaller encoding are used).  Usage Frequency One more interesting way to visualize the codespace is to look at the distribution of usage—in other words, how often each code point is actually used in real-world texts. Below is a heat map of planes 0–2 based on a large sample of text from Wikipedia and Twitter (all languages). Frequency increases from black (never seen) through red and yellow to white.  Heat map of code point usage frequency in Unicode planes 0–2 (click to zoom)  You can see that the vast majority of this text sample lies in the BMP, with only scattered usage of code points from planes 1–2. The biggest exception is emoji, which show up here as the several bright squares in the bottom row of plane 1.  Encodings We’ve seen that Unicode code points are abstractly identified by their index in the codespace, ranging from U+0000 to U+10FFFF. But how do code points get represented as bytes, in memory or in a file?  The most convenient, computer-friendliest (and programmer-friendliest) thing to do would be to just store the code point index as a 32-bit integer. This works, but it consumes 4 bytes per code point, which is sort of a lot. Using 32-bit ints for Unicode will cost you a bunch of extra storage, memory, and performance in bandwidth-bound scenarios, if you work with a lot of text.  Consequently, there are several more-compact encodings for Unicode. The 32-bit integer encoding is officially called UTF-32 (UTF = “Unicode Transformation Format”), but it’s rarely used for storage. At most, it comes up sometimes as a temporary internal representation, for examining or operating on the code points in a string.  Much more commonly, you’ll see Unicode text encoded as either UTF-8 or UTF-16. These are both variable-length encodings, made up of 8-bit or 16-bit units, respectively. In these schemes, code points with smaller index values take up fewer bytes, which saves a lot of memory for typical texts. The trade-off is that processing UTF-8/16 texts is more programmatically involved, and likely slower.  UTF-8 In UTF-8, each code point is stored using 1 to 4 bytes, based on its index value.  UTF-8 uses a system of binary prefixes, in which the high bits of each byte mark whether it’s a single byte, the beginning of a multi-byte sequence, or a continuation byte; the remaining bits, concatenated, give the code point index. This table shows how it works:  UTF-8 (binary)\\tCode point (binary)\\tRange 0xxxxxxx\\txxxxxxx\\tU+0000–U+007F 110xxxxx 10yyyyyy\\txxxxxyyyyyy\\tU+0080–U+07FF 1110xxxx 10yyyyyy 10zzzzzz\\txxxxyyyyyyzzzzzz\\tU+0800–U+FFFF 11110xxx 10yyyyyy 10zzzzzz 10wwwwww\\txxxyyyyyyzzzzzzwwwwww\\tU+10000–U+10FFFF A handy property of UTF-8 is that code points below 128 (ASCII characters) are encoded as single bytes, and all non-ASCII code points are encoded using sequences of bytes 128–255. This has a couple of nice consequences. First, any strings or files out there that are already in ASCII can also be interpreted as UTF-8 without any conversion. Second, lots of widely-used string programming idioms—such as null termination, or delimiters (newlines, tabs, commas, slashes, etc.)—will just work on UTF-8 strings. ASCII bytes never occur inside the encoding of non-ASCII code points, so searching byte-wise for a null terminator or a delimiter will do the right thing.  Thanks to this convenience, it’s relatively simple to extend legacy ASCII programs and APIs to handle UTF-8 strings. UTF-8 is very widely used in the Unix/Linux and Web worlds, and many programmers argue UTF-8 should be the default encoding everywhere.  However, UTF-8 isn’t a drop-in replacement for ASCII strings in all respects. For instance, code that iterates over the “characters” in a string will need to decode UTF-8 and iterate over code points (or maybe grapheme clusters—more about those later), not bytes. When you measure the “length” of a string, you’ll need to think about whether you want the length in bytes, the length in code points, the width of the text when rendered, or something else.  UTF-16 The other encoding that you’re likely to encounter is UTF-16. It uses 16-bit words, with each code point stored as either 1 or 2 words.  Like UTF-8, we can express the UTF-16 encoding rules in the form of binary prefixes:  UTF-16 (binary)\\tCode point (binary)\\tRange xxxxxxxxxxxxxxxx\\txxxxxxxxxxxxxxxx\\tU+0000–U+FFFF 110110xxxxxxxxxx 110111yyyyyyyyyy\\txxxxxxxxxxyyyyyyyyyy + 0x10000\\tU+10000–U+10FFFF A more common way that people talk about UTF-16 encoding, though, is in terms of code points called “surrogates”. All the code points in the range U+D800–U+DFFF—or in other words, the code points that match the binary prefixes 110110 and 110111 in the table above—are reserved specifically for UTF-16 encoding, and don’t represent any valid characters on their own. They’re only meant to occur in the 2-word encoding pattern above, which is called a “surrogate pair”. Surrogate code points are illegal in any other context! They’re not allowed in UTF-8 or UTF-32 at all.  Historically, UTF-16 is a descendant of the original, pre-1996 versions of Unicode, in which there were only 65,536 code points. The original intention was that there would be no different “encodings”; Unicode was supposed to be a straightforward 16-bit character set. Later, the codespace was expanded to make room for a long tail of less-common (but still important) Han characters, which the Unicode designers didn’t originally plan for. Surrogates were then introduced, as—to put it bluntly—a kludge, allowing 16-bit encodings to access the new code points.  Today, Javascript uses UTF-16 as its standard string representation: if you ask for the length of a string, or iterate over it, etc., the result will be in UTF-16 words, with any code points outside the BMP expressed as surrogate pairs. UTF-16 is also used by the Microsoft Win32 APIs; though Win32 supports either 8-bit or 16-bit strings, the 8-bit version unaccountably still doesn’t support UTF-8—only legacy code-page encodings, like ANSI. This leaves UTF-16 as the only way to get proper Unicode support in Windows. (Update: in Win10 version 1903, they finally added UTF-8 support to the 8-bit APIs! 😊)  By the way, UTF-16’s words can be stored either little-endian or big-endian. Unicode has no opinion on that issue, though it does encourage the convention of putting U+FEFF zero width no-break space at the top of a UTF-16 file as a byte-order mark, to disambiguate the endianness. (If the file doesn’t match the system’s endianness, the BOM will be decoded as U+FFFE, which isn’t a valid code point.)  Combining Marks In the story so far, we’ve been focusing on code points. But in Unicode, a “character” can be more complicated than just an individual code point!  Unicode includes a system for dynamically composing characters, by combining multiple code points together. This is used in various ways to gain flexibility without causing a huge combinatorial explosion in the number of code points.  In European languages, for example, this shows up in the application of diacritics to letters. Unicode supports a wide range of diacritics, including acute and grave accents, umlauts, cedillas, and many more. All these diacritics can be applied to any letter of any alphabet—and in fact, multiple diacritics can be used on a single letter.  If Unicode tried to assign a distinct code point to every possible combination of letter and diacritics, things would rapidly get out of hand. Instead, the dynamic composition system enables you to construct the character you want, by starting with a base code point (the letter) and appending additional code points, called “combining marks”, to specify the diacritics. When a text renderer sees a sequence like this in a string, it automatically stacks the diacritics over or under the base letter to create a composed character.  For example, the accented character “Á” can be expressed as a string of two code points: U+0041 “A” latin capital letter a plus U+0301 “◌́” combining acute accent. This string automatically gets rendered as a single character: “Á”.  Now, Unicode does also include many “precomposed” code points, each representing a letter with some combination of diacritics already applied, such as U+00C1 “Á” latin capital letter a with acute or U+1EC7 “ệ” latin small letter e with circumflex and dot below. I suspect these are mostly inherited from older encodings that were assimilated into Unicode, and kept around for compatibility. In practice, there are precomposed code points for most of the common letter-with-diacritic combinations in European-script languages, so they don’t use dynamic composition that much in typical text.  Still, the system of combining marks does allow for an arbitrary number of diacritics to be stacked on any base character. The reductio-ad-absurdum of this is Zalgo text, which works by ͖͟ͅr͞aṋ̫̠̖͈̗d͖̻̹óm̪͙͕̗̝ļ͇̰͓̳̫ý͓̥̟͍ ̕s̫t̫̱͕̗̰̼̘͜a̼̩͖͇̠͈̣͝c̙͍k̖̱̹͍͘i̢n̨̺̝͇͇̟͙ģ̫̮͎̻̟ͅ ̕n̼̺͈͞u̮͙m̺̭̟̗͞e̞͓̰̤͓̫r̵o̖ṷs҉̪͍̭̬̝̤ ̮͉̝̞̗̟͠d̴̟̜̱͕͚i͇̫̼̯̭̜͡ḁ͙̻̼c̲̲̹r̨̠̹̣̰̦i̱t̤̻̤͍͙̘̕i̵̜̭̤̱͎c̵s ͘o̱̲͈̙͖͇̲͢n͘ ̜͈e̬̲̠̩ac͕̺̠͉h̷̪ ̺̣͖̱ḻ̫̬̝̹ḙ̙̺͙̭͓̲t̞̞͇̲͉͍t̷͔̪͉̲̻̠͙e̦̻͈͉͇r͇̭̭̬͖,̖́ ̜͙͓̣̭s̘̘͈o̱̰̤̲ͅ ̛̬̜̙t̼̦͕̱̹͕̥h̳̲͈͝ͅa̦t̻̲ ̻̟̭̦̖t̛̰̩h̠͕̳̝̫͕e͈̤̘͖̞͘y҉̝͙ ̷͉͔̰̠o̞̰v͈͈̳̘͜er̶f̰͈͔ḻ͕̘̫̺̲o̲̭͙͠ͅw̱̳̺ ͜t̸h͇̭͕̳͍e̖̯̟̠ ͍̞̜͔̩̪͜ļ͎̪̲͚i̝̲̹̙̩̹n̨̦̩̖ḙ̼̲̼͢ͅ ̬͝s̼͚̘̞͝p͙̘̻a̙c҉͉̜̤͈̯̖i̥͡n̦̠̱͟g̸̗̻̦̭̮̟ͅ ̳̪̠͖̳̯̕a̫͜n͝d͡ ̣̦̙ͅc̪̗r̴͙̮̦̹̳e͇͚̞͔̹̫͟a̙̺̙ț͔͎̘̹ͅe̥̩͍ a͖̪̜̮͙̹n̢͉̝ ͇͉͓̦̼́a̳͖̪̤̱p̖͔͔̟͇͎͠p̱͍̺ę̲͎͈̰̲̤̫a̯͜r̨̮̫̣̘a̩̯͖n̹̦̰͎̣̞̞c̨̦̱͔͎͍͖e̬͓͘ ̤̰̩͙̤̬͙o̵̼̻̬̻͇̮̪f̴ ̡̙̭͓͖̪̤“̸͙̠̼c̳̗͜o͏̼͙͔̮r̞̫̺̞̥̬ru̺̻̯͉̭̻̯p̰̥͓̣̫̙̤͢t̳͍̳̖ͅi̶͈̝͙̼̙̹o̡͔n̙̺̹̖̩͝ͅ”̨̗͖͚̩.̯͓  A few other places where dynamic character composition shows up in Unicode:  Vowel-pointing notation in Arabic and Hebrew. In these languages, words are normally spelled with some of their vowels left out. They then have diacritic notation to indicate the vowels (used in dictionaries, language-teaching materials, children’s books, and such). These diacritics are expressed with combining marks.  A Hebrew example, with niqqud:\\tאֶת דַלְתִּי הֵזִיז הֵנִיעַ, קֶטֶב לִשְׁכַּתִּי יָשׁוֹד Normal writing (no niqqud):\\tאת דלתי הזיז הניע, קטב לשכתי ישוד Devanagari, the script used to write Hindi, Sanskrit, and many other South Asian languages, expresses certain vowels as combining marks attached to consonant letters. For example, “ह” + “\\u200bि” = “हि” (“h” + “i” = “hi”). Korean characters stand for syllables, but they are composed of letters called jamo that stand for the vowels and consonants in the syllable. While there are code points for precomposed Korean syllables, it’s also possible to dynamically compose them by concatenating their jamo. For example, “ᄒ” + “ᅡ” + “ᆫ” = “한” (“h” + “a” + “n” = “han”). Canonical Equivalence In Unicode, precomposed characters exist alongside the dynamic composition system. A consequence of this is that there are multiple ways to express “the same” string—different sequences of code points that result in the same user-perceived characters. For example, as we saw earlier, we can express the character “Á” either as the single code point U+00C1, or as the string of two code points U+0041 U+0301.  Another source of ambiguity is the ordering of multiple diacritics in a single character. Diacritic order matters visually when two diacritics apply to the same side of the base character, e.g. both above: “ǡ” (dot, then macron) is different from “ā̇” (macron, then dot). However, when diacritics apply to different sides of the character, e.g. one above and one below, then the order doesn’t affect rendering. Moreover, a character with multiple diacritics might have one of the diacritics precomposed and others expressed as combining marks.  For example, the Vietnamese letter “ệ” can be expressed in five different ways:  Fully precomposed: U+1EC7 “ệ” Partially precomposed: U+1EB9 “ẹ” + U+0302 “◌̂” Partially precomposed: U+00EA “ê” + U+0323 “◌̣” Fully decomposed: U+0065 “e” + U+0323 “◌̣” + U+0302 “◌̂” Fully decomposed: U+0065 “e” + U+0302 “◌̂” + U+0323 “◌̣” Unicode refers to set of strings like this as “canonically equivalent”. Canonically equivalent strings are supposed to be treated as identical for purposes of searching, sorting, rendering, text selection, and so on. This has implications for how you implement operations on text. For example, if an app has a “find in file” operation and the user searches for “ệ”, it should, by default, find occurrences of any of the five versions of “ệ” above!  Normalization Forms To address the problem of “how to handle canonically equivalent strings”, Unicode defines several normalization forms: ways of converting strings into a canonical form so that they can be compared code-point-by-code-point (or byte-by-byte).  The “NFD” normalization form fully decomposes every character down to its component base and combining marks, taking apart any precomposed code points in the string. It also sorts the combining marks in each character according to their rendered position, so e.g. diacritics that go below the character come before the ones that go above the character. (It doesn’t reorder diacritics in the same rendered position, since their order matters visually, as previously mentioned.)  The “NFC” form, conversely, puts things back together into precomposed code points as much as possible. If an unusual combination of diacritics is called for, there may not be any precomposed code point for it, in which case NFC still precomposes what it can and leaves any remaining combining marks in place (again ordered by rendered position, as in NFD).  There are also forms called NFKD and NFKC. The “K” here refers to compatibility decompositions, which cover characters that are “similar” in some sense but not visually identical. However, I’m not going to cover that here.  Grapheme Clusters As we’ve seen, Unicode contains various cases where a thing that a user thinks of as a single “character” might actually be made up of multiple code points under the hood. Unicode formalizes this using the notion of a grapheme cluster: a string of one or more code points that constitute a single “user-perceived character”.  UAX #29 defines the rules for what, precisely, qualifies as a grapheme cluster. It’s approximately “a base code point followed by any number of combining marks”, but the actual definition is a bit more complicated; it accounts for things like Korean jamo, and emoji ZWJ sequences.  The main thing grapheme clusters are used for is text editing: they’re often the most sensible unit for cursor placement and text selection boundaries. Using grapheme clusters for these purposes ensures that you can’t accidentally chop off some diacritics when you copy-and-paste text, that left/right arrow keys always move the cursor by one visible character, and so on.  Another place where grapheme clusters are useful is in enforcing a string length limit—say, on a database field. While the true, underlying limit might be something like the byte length of the string in UTF-8, you wouldn’t want to enforce that by just truncating bytes. At a minimum, you’d want to “round down” to the nearest code point boundary; but even better, round down to the nearest grapheme cluster boundary. Otherwise, you might be corrupting the last character by cutting off a diacritic, or interrupting a jamo sequence or ZWJ sequence.  And More… There’s much more that could be said about Unicode from a programmer’s perspective! I haven’t gotten into such fun topics as case mapping, collation, compatibility decompositions and confusables, Unicode-aware regexes, or bidirectional text. Nor have I said anything yet about implementation issues—how to efficiently store and look-up data about the sparsely-assigned code points, or how to optimize UTF-8 decoding, string comparison, or NFC normalization. Perhaps I’ll return to some of those things in future posts.  Unicode is a fascinating and complex system. It has a many-to-one mapping between bytes and code points, and on top of that a many-to-one (or, under some circumstances, many-to-many) mapping between code points and “characters”. It has oddball special cases in every corner. But no one ever claimed that representing all written languages was going to be easy, and it’s clear that we’re never going back to the bad old days of a patchwork of incompatible encodings.  Further reading:  The Unicode Standard UTF-8 Everywhere Manifesto Dark corners of Unicode by Eevee ICU (International Components for Unicode)—C/C++/Java libraries implementing many Unicode algorithms and related things Python 3 Unicode Howto Google Noto Fonts—set of fonts intended to cover all assigned code points\"\"\"\n",
    "\n",
    "tokens = text.encode(\"utf-8\")  # raw bytes\n",
    "tokens = list(\n",
    "    map(int, tokens)\n",
    ")  # convert to a list of integers in range 0..255 for convenience\n",
    "\n",
    "\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):  # Pythonic way to iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    # In the list of ints (ids), replace all consecutive\n",
    "    # occurences of pair with the new token id\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # If we are not at the very last position AND the pair matches,\n",
    "        # replace it with the new token id\n",
    "        if (i < len(ids) - 1) and (ids[i] == pair[0] and ids[i + 1] == pair[1]):\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "\n",
    "    return newids\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "vocab_size = 276  # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256  # we have 256 tokens for the raw bytes\n",
    "ids = list(tokens)  # copy so we don't destroy the original list\n",
    "\n",
    "merges = {}  # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    best_pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"Merging pair {best_pair} into a new token {idx}\")\n",
    "    ids = merge(ids, best_pair, idx)\n",
    "    merges[best_pair] = idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Another thing to note is that the new tokens, after merging, will also be available for subsequent merging.\n",
    "\n",
    "Let's take a look at the compression ratio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tokens length\", len(tokens))\n",
    "print(\"ids length\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens)/len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more vocabulary you add, the compression ratio increases because we would have more merges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer <a id=\"tokenizer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the `Tokenizer` is a completely separate, independent module from the LLM. It has its own training dataset of text (which could be different from that of the LLM), on which you train the vocabulary using the Byte Pair Encoding (BPE) algorithm. It then translates back and forth between raw text and sequences of tokens. The LLM later only ever sees the tokens and never directly deals with any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Screenshot 2024-02-14 at 8.23.33 AM.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding <a id=\"decoding\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a sequence of integers in the range [0, vocab_size], what is the text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys are token IDs and values are raw bytes\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "# And now going up the merge tree (adding the merges based on the merge rules)\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "\n",
    "def decode(ids):\n",
    "    # Given a list of ints (ids), return Python str\n",
    "    tokens = b\"\".join([vocab[idx] for idx in ids])\n",
    "    # Tokens are raw bytes so we have to decode to a string\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "\n",
    "print(\n",
    "    decode(\n",
    "        [\n",
    "            236,\n",
    "            149,\n",
    "            136,\n",
    "            235,\n",
    "            133,\n",
    "            149,\n",
    "            237,\n",
    "            149,\n",
    "            152,\n",
    "            236,\n",
    "            132,\n",
    "            184,\n",
    "            236,\n",
    "            154,\n",
    "            148,\n",
    "            32,\n",
    "            240,\n",
    "            159,\n",
    "            145,\n",
    "            139,\n",
    "            32,\n",
    "            40,\n",
    "            104,\n",
    "            101,\n",
    "            108,\n",
    "            108,\n",
    "            111,\n",
    "            32,\n",
    "            105,\n",
    "            110,\n",
    "            32,\n",
    "            75,\n",
    "            111,\n",
    "            114,\n",
    "            101,\n",
    "            97,\n",
    "            110,\n",
    "            33,\n",
    "            41,\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode([128]))  # this will throw an error if `errors=\"strict\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding <a id=\"encoding\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the other way around: Given a string, what are the tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    # Given a str, return a list of ints (tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))  # raw bytes\n",
    "    while len(tokens) >= 2:\n",
    "        # We only care about the keys (pairs) here not the values (counts)\n",
    "        stats = get_stats(tokens)\n",
    "        # Fancy code explained below in detail\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break  # because nothing to merge\n",
    "        idx = merges[pair]\n",
    "        # Perform the merge\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "print(encode(\"hello world!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fancy code explanation <a id=\"fancy\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the fancy code, we are interested in finding the pair (key) in `stats` with the lowest index in the `merges` dictionary. This is because we want to do all the early merges before we reach the later merges. Let me explain in more detail. Assume the following merges:\n",
    "\n",
    "\n",
    "```python\n",
    "print(merges)\n",
    "\n",
    "{(101, 32): 256,  # we need to merge this first\n",
    " (105, 110): 257,\n",
    " (115, 32): 258,\n",
    " (116, 104): 259,  # then this\n",
    " (101, 114): 260,\n",
    " (99, 111): 261,\n",
    " (116, 32): 262,\n",
    " (226, 128): 263,\n",
    " (44, 32): 264,\n",
    " (97, 110): 265,\n",
    " (111, 114): 266,\n",
    " (100, 32): 267,\n",
    " (97, 114): 268,\n",
    " (101, 110): 269,\n",
    " (257, 103): 270,\n",
    " (261, 100): 271,\n",
    " (121, 32): 272,\n",
    " (46, 32): 273,\n",
    " (97, 108): 274,\n",
    " (259, 256): 275}  # before we can do this\n",
    "```\n",
    "\n",
    "Before we can merge tokens 259 and 256 to give us token 275, we would have to merge (116, 104) and (101, 32) respectively. This is why we want to do all the early merges first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain the fancy code above in more detail, assume the following example. Imagine we have a text we'd like to encode. We first use the `get_stats` function to get the `stats` dictionary where the keys are consecutive pairs and the values the counts.\n",
    "\n",
    "```python\n",
    "text = \"hello world!\"\n",
    "tokens = list(text.encode(\"utf-8\"))  # raw bytes\n",
    "stats = get_stats(tokens)\n",
    "print(stats)\n",
    "```\n",
    "\n",
    "```python\n",
    "# output\n",
    "{(104, 101): 1,\n",
    " (101, 108): 1,\n",
    " (108, 108): 1,\n",
    " (108, 111): 1,\n",
    " (111, 32): 1,\n",
    " (32, 119): 1,\n",
    " (119, 111): 1,\n",
    " (111, 114): 1,\n",
    " (114, 108): 1,\n",
    " (108, 100): 1,\n",
    " (100, 33): 1}\n",
    "```\n",
    "\n",
    "Now how do we find the pair (key) in `stats` that has the lowest index in the `merges` dictionary (remember, we do this to do the early merges first before the later merges)? We could do the following:\n",
    "\n",
    "1. We can iterate through the pairs (keys) in `stats` (we don't care about the counts here)\n",
    "\n",
    "2. For each pair, we check if the pair is in the `merges` dictionary\n",
    "\n",
    "    * If the pair is present, we return the value in the `merges` dictionary\n",
    "    \n",
    "    * If the pair is **not** present, we return a arbitarily large value like infinity\n",
    "\n",
    "3. We append the result in step 2 to a temporary list\n",
    "\n",
    "4. Once the iteration is completed, we take the index in the temporary list corresponding to the minimum value\n",
    "\n",
    "5. Using this index, we can get the corresponding pair in the `stats` dictionary\n",
    "\n",
    "Here's the code in full:\n",
    "\n",
    "```python\n",
    "tmp = []\n",
    "for k in stats.keys():                      # Step 1 -> we don't care about the counts (values) because we are not training the tokenizer now\n",
    "    value = merges.get(k, float(\"inf\"))     # Step 2\n",
    "    tmp.append(value)                       # Step 3\n",
    "\n",
    "idx_of_min = tmp.index(min(tmp))            # Step 4 -> this step ensures we go from first to last in order of merge rule\n",
    "pair = list(stats.keys())[idx_of_min]       # Step 5\n",
    "    \n",
    "print(\"---\")\n",
    "print(idx_of_min)  # 7\n",
    "print(\"---\")\n",
    "print(pair)  # (111, 114) -> we will use this pair (key) to get the new index (value) from the merge rule\n",
    "```\n",
    "\n",
    "A less verbose version of the code above is the fancy code:\n",
    "\n",
    "```python\n",
    "pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))  # fancy code of the verbose version above\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(encode(\"hello world!\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we encode and string and decode it back, we'd expect to get the same string back. However, this is not true for all strings. This is because not all token sequences are valid UTF-8 byte streams. Therefore, some of them can't even be decoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forced splits using regex patterns (GPT Series) <a id=\"forced_splits\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GPT-2, they did not only apply the BPE algorithm as naively as we have done so above. Suppose that we have common words like `dog` and it happens frequently in the text. What will happen in our naive BPE implementation is that we will find the word `dog` occurring together with all kinds of punctuations such as `dog.`, `dog!` and `dog?`. It is obvious that the BPE algorithm should be merging these words together when in fact, it is combining semantics with punctuations. This is suboptimal.\n",
    "\n",
    "To tackle this, the authors enforced that some types of characters should never be merged together. This was what was introduced on top of the BPE algorithm. Let's have a look at a [snippet of their code](https://github.com/openai/gpt-2/blob/9b63575ef42771a015060c964af2c3da4cf7c8ab/src/encoder.py#L53C9-L53C113) which implements this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "\n",
    "\n",
    "gpt2pat = re.compile(\n",
    "    r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "print(re.findall(gpt2pat, \"Hello world how are you\"))  # triggers r\" ?\\p{L}+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our text based on the regex pattern above. More specifically, we are taking our text, and instead of directly encoding it for tokenization, we are first splitting up the text into a list of texts. Every element in the list are then processed independently by the tokenizer and the results of the tokenization are concatenated. \n",
    "\n",
    "Roughly speaking, what the above does is that we're ever only finding merges between the elements of the list instead of across words in the entire text. What this means is that the letter \"e\" in \"are\" will not be joined with the whitespace in \" you\" because now they are part of separate elements in the list.\n",
    "\n",
    "Therefore, using this regex pattern to chunk up the text is one way of enforcing that some merges are not to happen.\n",
    "\n",
    "**Note:** The above text triggers the `r\" ?\\p{L}+\"` pattern in the regex pattern. What does the pattern mean and why is it important? The pattern means that we will split when we see an optional space followed by any number of letters (a word)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do numbers get captured by the regex pattern? <a id=\"numbers\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the text below, the word \" world123\" will trigger both `r\" ?\\p{L}+\"` and `r\" ?\\p{N}+\"`. The word \" world\" has a space followed by some number of letters and is captured by `r\" ?\\p{L}+\"` whereas the word \"123\" is captured by `r\" ?\\p{N}+\"` since it does not have a leading space and is followed by some number of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    re.findall(gpt2pat, \"Hello world123 how are you\")\n",
    ")  # triggers r\" ?\\p{L}+\" and r\" ?\\p{N}+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do apostrophes get captured by the regex pattern? <a id=\"apostrophes\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the text below, the words \"Hello've\" and \" how's\" will trigger both `r\"'ve\"` and `r\"'s\"`. This means the regex will capture the word \"Hello\" followed by \"'ve\" and the word \" how\" followed by \"'s\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(gpt2pat, \"Hello've world how's are you\"))  # triggers r\"'ve\" and r\"'s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to note that these apostrophes are hardcoded and do not work if you use the Unicode apostrophe. This means the word \"how’s\" will be split into \"how\", \"’\" and \"s\" (which seems nonsensical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(gpt2pat, \"Hello’ve world how’s are you\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to note is that the authors did not add `re.IGNORECASE` which means that apostrophes followed by capital letters will not be captured. See example below. Similar to the case above, it seems like the splitting resulted in nonsensical lone characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(gpt2pat, \"Hello'VE world how'S are you\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do punctuations get captured by the regex pattern? <a id=\"punctuations\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regex `r\" ?[^\\s\\p{L}\\p{N}]+\"` signifies that if there is an optional space followed by neither a word, number nor whitespace, then it must be a punctuation. See the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(gpt2pat, \"Hello world how are you!!!???\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do unecessary whitespaces get captured by the regex pattern? <a id=\"whitespaces\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the regex `r\"\\s+(?!\\S)|\\s+\"` captures whitespaces up until but not including the last whitespace. See the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(gpt2pat, \"Hello world how are                     you\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Python code <a id=\"example-python\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the example Python code below has many elements that are captured by the regex pattern. That is because the regex pattern is splitting up the text every time a category changes (sort of). Therefore, there will never be any merges between the elements of the list.\n",
    "\n",
    "You might think that in order to train the tokenizer, OpenAI used the same regex pattern so split up the text into chunks and then ran the BPE algorithm within all the chunks. However, that is not exactly entirely what happened. \n",
    "\n",
    "In the code cell below, the whitespaces from the Python code got split up by the regex pattern into elements but these spaces never ended up being merged together. You can confirm this by copying the following example Python code and pasting it into the [Tiktokenizer app](https://tiktokenizer.vercel.app/). You can see that the white spaces ended up having the token ID of 220. This meant that at some point, OpenAI enforced some rule that these spaces would never be merged. \n",
    "\n",
    "There are some additional rules on top of simply chunking and running BPE that OpenAI is unclear about. The training code for the BPE was never released by OpenAI, only the inference code. Therefore, we can't be sure how the training was done but it is not as simple as chunking the text and running BPE on the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"\"\"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "\"\"\"\n",
    "\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tiktoken <a id=\"tiktoken\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "220 are the token ID for whitespaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference the GPT-2 [encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "Download the vocab.bpe and encoder.json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "# !wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"encoder.json\", \"r\") as f:\n",
    "    encoder = json.load(f)  # <--- ~equivalent to our \"vocab\"\n",
    "\n",
    "with open(\"vocab.bpe\", \"r\", encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "\n",
    "bpe_merges = [\n",
    "    tuple(merge_str.split()) for merge_str in bpe_data.split(\"\\n\")[1:-1]\n",
    "]  # <---- ~equivalent to our \"merges\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Tokens <a id=\"special-tokens\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to tokens that are coming from the raw bytes and BPE merges, we can also insert all kinds of tokens that will be used to delimit different parts of the data or introduced to create a special structure of the tokens streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `encoder` object from OpenAI's GPT-2 (which is equivalent to our `vocab`) has a length of 50,257. Where does this number come from?\n",
    "\n",
    "* 256 raw byte tokens\n",
    "* 50,000 merges\n",
    "* 1 special token\n",
    "\n",
    "What is this special token?\n",
    "\n",
    "```python\n",
    "print(encoder['<|endoftext|>'])  # 50256\n",
    "```\n",
    "\n",
    "This special token is used to delimit documents. When we are creating the training data, we have all these documents which we tokenize and get a stream of tokens. Those tokens only range from 0 to 50,256 and in between those documents, we insert special end-of-text tokens. We are doing this as a signal to the language model that the document has ended and what follows is going to be unrelated to the previous document. The language model has to learn this from data. It needs to learn that this token means that it should treat the previous and next document separate after this special token. In a way, it should \"wipe its memory\" regarding the previous document because it will not be informative to what comes next in the next document.\n",
    "\n",
    "\n",
    "**Note:** In our `vocab`, we map integers to raw bytes. If you take a look into the [`encoder.json`](minbpe/notebooks/encoder.json), it maps from strings to integers instead for no amazing reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder[\"<|endoftext|>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise <a id=\"exercise\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you have everything you need to build your own GPT-4 tokenizer. This is the [exercise progression](https://github.com/karpathy/minbpe/blob/master/exercise.md) you may wish to follow. You'll note that it is part of the [minbpe](https://github.com/karpathy/minbpe) repo, which is the solution to that exercise, and is a cleaned up version of the code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 tokenizer\n",
    "print(enc.encode(\"안녕하세요 👋 (hello in Korean!)\"))\n",
    "print(\n",
    "    enc.decode(enc.encode(\"안녕하세요 👋 (hello in Korean!)\")) == \"안녕하세요 👋 (hello in Korean!)\"\n",
    ")\n",
    "# Match the above for your own tokenizer, and also implement a train() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentencepiece <a id=\"sentencepiece\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commonly used because (unlike tiktoken) it can efficiently both train and inference BPE tokenizers. It is used in both Llama and Mistral series.\n",
    "\n",
    "[`sentencepiece` on Github link](https://github.com/google/sentencepiece).\n",
    "\n",
    "**The big difference**: `sentencepiece` runs the BPE algorithm on the Unicode code points directly! It then has an option `character_coverage` for what to do with very very rare code points that appear very few times, and it either maps them onto an `UNK` token, or if `byte_fallback` is turned on, it encodes them with UTF-8 into bytes and then encodes the raw bytes instead.\n",
    "\n",
    "TLDR:\n",
    "\n",
    "* `tiktoken` takes the Unicode code points, encodes them into bytes using UTF-8 and then performs the BPE algorithm on those bytes\n",
    "* `sentencepiece` runs the BPE algorithm the Unicode code points and optionally falls back to UTF-8 bytes for rare Unicode code points (rarity is determined by `character_coverage` hyperparameter), which then get translated to byte tokens\n",
    "\n",
    "(Personally I think the tiktoken way is a lot cleaner...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "# Write a toy.txt file with some random text\n",
    "with open(\"tests/toy.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\n",
    "        \"SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docs for `sentencepiece` options:\n",
    "\n",
    "* [markdown](https://github.com/google/sentencepiece/blob/master/doc/options.md)\n",
    "* [protobuf](https://github.com/google/sentencepiece/blob/master/src/sentencepiece_model.proto#L193)\n",
    "\n",
    "There is a ton of options and configurations when using `sentencepiece` and the reason this is so is because it's been around for quite some time and it tries to handle a large diversity of things. Because of this, it has quite a bit of accumulated historical baggage.\n",
    "\n",
    "**Note:** Many of the configurations however, are irrelevant to the BPE algorithm (e.g. `shrinking_factor` applies to a different training algorithm entirely)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a sentencepiece model on it\n",
    "# The settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "\n",
    "options = dict(\n",
    "    # input spec\n",
    "    input=\"tests/toy.txt\",\n",
    "    input_format=\"text\",\n",
    "    # output spec\n",
    "    model_prefix=\"tok400\",  # output filename prefix\n",
    "    # algorithm spec\n",
    "    # BPE alg\n",
    "    model_type=\"bpe\",\n",
    "    vocab_size=400,\n",
    "    # normalization\n",
    "    normalization_rule_name=\"identity\",  # ew, turn off normalization\n",
    "    remove_extra_whitespaces=False,\n",
    "    input_sentence_size=200000000,  # max number of training sentences\n",
    "    max_sentence_length=4192,  # max number of bytes per sentence\n",
    "    seed_sentencepiece_size=1000000,\n",
    "    shuffle_input_sentence=True,\n",
    "    # rare word treatment\n",
    "    character_coverage=0.99995,\n",
    "    byte_fallback=True,\n",
    "    # merge rules\n",
    "    split_digits=True,\n",
    "    split_by_unicode_script=True,\n",
    "    split_by_whitespace=True,\n",
    "    split_by_number=True,\n",
    "    max_sentencepiece_length=16,\n",
    "    add_dummy_prefix=True,\n",
    "    allow_whitespace_only_pieces=True,\n",
    "    # special tokens\n",
    "    unk_id=0,  # the UNK token MUST exist\n",
    "    bos_id=1,  # the others are optional, set to -1 to turn off\n",
    "    eos_id=2,\n",
    "    pad_id=-1,\n",
    "    # systems\n",
    "    num_threads=os.cpu_count(),  # use ~all system resources\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train `sentencepiece`\n",
    "spm.SentencePieceTrainer.train(**options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train `sentencepiece` on our text, it will generate the `tok400.model` and `tok400.vocab` files. We can then load the `tok400.model` file and inspect the vocabulary. \n",
    "\n",
    "**Note*:* We trained a `vocab_size` of 400 which is why we have 400 tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the individual tokens `sentencepiece` will create. In the beginning, we can see the `unk` token with the ID 0. Then we have the beginning of sequence (`bos`) and end of sequence (`eos`) with IDs 1 and 2 respectively. Because we set the `pad_id` (see configuration above) to -1, we don't have a padding token.\n",
    "\n",
    "Following the special tokens, we have the 256 byte tokens and their IDs. After the byte tokens, we have the merges (i.e. the parent nodes in the merges) and their IDs. Finally, after the merges, come the individual tokens (the Unicode code point tokens if you will) and their IDs.\n",
    "\n",
    "This is the ordering with which `sentencepiece` represents its vocabularies. It starts with special tokens, byte tokens, merge tokens, and then individual Unicode code point (raw) tokens. All these raw Unicode code point tokens are the ones it encountered in the [training set](../tests/toy.txt). These raw tokens are not considered extremely rare as determined by `character_coverage` in the configuration. If they were, then they will not be included in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"tok400.model\")\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `byte_fallback` <a id=\"byte-fallback\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = sp.encode(\"hello 안녕하세요\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Korean characters are not part of the [training set](../tests/toy.txt) so `sentencepiece` is encountering code points that it has not seen during training time and those code points do not have a token associated to them. Now suddenly, these Korean characters are `unk` tokens but since `byte_fallback` is enabled to `True`, `sentencepiece` instead falls back to bytes. So instead, it takes the Korean characters, encodes it using UTF-8 into raw bytes and uses the byte tokens in the vocabulary to represent those raw bytes.\n",
    "\n",
    "**Note:** If `byte_fallback` is set to `False` then `sentencepiece` will not fall back to bytes and there will be more merge tokens since now the vocabulary has more space. Then now, if you encode the Korean characters, it simply becomes a 0 (i.e. `unk` token). It is important to note that these 0's will be fed into your language model. So what is a language model supposed to do when all kinds of different things that are unrecognized because they are rare just end up mapping to `unk`s? This definitely isn't the property that you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `add_dummy_prefix` <a id=\"add-dummy-prefix\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([sp.id_to_piece(idx) for idx in ids])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to note when decoding the individual tokens is that the spaces end up being a bold underline (or underscore). More interestingly than that is that there is a bold underscore at the start of a word. That comes from the argument in the configuration called `add_dummy_prefix` which is set to `True`. In [protobuf](https://github.com/google/sentencepiece/blob/52a7f156a494ce288e357f9c4a3b1f955fc1014d/src/sentencepiece_model.proto#L256C3-L256C55) documentation, it describes the purpose of this argument. It adds dummy whitespace at the beginning of text in order to treat \"world\" in \"world\" and \"hello world\" in the same way.\n",
    "\n",
    "If you head back to the [Tiktokenizer app](https://tiktokenizer.vercel.app/), make sure `cl100k_base` (i.e. tokenizer for GPT-4) is selected. Then, if you type \"world\" and \"hello world\" into the text box, you will see that the word \"world\" by itself, has a different token ID than the word \"world\" in \"hello world\". The language model then has to learn from data that thet are actually kind of a very similar concept. Therefore, words in the beginning of sentences and words in the middle of sentences actually look completely different to a language model which uses `tiktoken` (but has been learned by the lanuage model that they are roughly the same).\n",
    "\n",
    "Therefore, this `add_dummy_prefix` is trying to fight this. The way that it works is that it basically adds a dummy prefix. As a part of preprocessing, it will take the string and it will add a whitespace at the beginning of the sentence in an effort to make words appearing in different parts of the sentences the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama 2 tokenizer proto <a id=\"llama-2-tokenizer-proto\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'd like to export the raw protocol buffer for the `tokenizer.model` released by meta, this is a [helpful issue](https://github.com/google/sentencepiece/issues/121). And this is the result:\n",
    "\n",
    "```\n",
    "normalizer_spec {\n",
    "  name: \"identity\"\n",
    "  precompiled_charsmap: \"\"\n",
    "  add_dummy_prefix: true\n",
    "  remove_extra_whitespaces: false\n",
    "  normalization_rule_tsv: \"\"\n",
    "}\n",
    "\n",
    "trainer_spec {\n",
    "  input: \"/large_experiments/theorem/datasets/MERGED/all.test1.merged\"\n",
    "  model_prefix: \"spm_model_32k_200M_charcov099995_allowWSO__v2\"\n",
    "  model_type: BPE\n",
    "  vocab_size: 32000\n",
    "  self_test_sample_size: 0\n",
    "  input_format: \"text\"\n",
    "  character_coverage: 0.99995\n",
    "  input_sentence_size: 200000000\n",
    "  seed_sentencepiece_size: 1000000\n",
    "  shrinking_factor: 0.75\n",
    "  num_threads: 80\n",
    "  num_sub_iterations: 2\n",
    "  max_sentence_length: 4192\n",
    "  shuffle_input_sentence: true\n",
    "  max_sentencepiece_length: 16\n",
    "  split_by_unicode_script: true\n",
    "  split_by_whitespace: true\n",
    "  split_by_number: true\n",
    "  treat_whitespace_as_suffix: false\n",
    "  split_digits: true\n",
    "  allow_whitespace_only_pieces: true\n",
    "  vocabulary_output_piece_score: true\n",
    "  hard_vocab_limit: true\n",
    "  use_all_vocab: false\n",
    "  byte_fallback: true\n",
    "  required_chars: \"\"\n",
    "  unk_id: 0\n",
    "  bos_id: 1\n",
    "  eos_id: 2\n",
    "  pad_id: -1\n",
    "  unk_surface: \" \\342\\201\\207 \"\n",
    "  unk_piece: \"<unk>\"\n",
    "  bos_piece: \"<s>\"\n",
    "  eos_piece: \"</s>\"\n",
    "  pad_piece: \"<pad>\"\n",
    "  train_extremely_large_corpus: false\n",
    "  enable_differential_privacy: false\n",
    "  differential_privacy_noise_level: 0.0\n",
    "  differential_privacy_clipping_threshold: 0\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `vocab_size` <a id=\"vocab-size\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Question:** What should be `vocab_size`?\n",
    "* **Question:** How can I increase `vocab_size`?\n",
    "* **Answer:** Let's see. Reminder: `gpt.py` from before.\n",
    "\n",
    "`vocab_size` in `gpt.py` does not come up much in most of the layers. The only two places that it comes up to is in exactly two places:\n",
    "\n",
    "1. `nn.Embedding` \n",
    "    * As `vocab_size` grows, the embedding table also increases in size\n",
    "\n",
    "2. `nn.Linear` \n",
    "    * Linear layer used at the end of the Transformer and is used to produce the logits which will become the probabilities for the next token in the sequence\n",
    "\n",
    "```python\n",
    "# each token directly reads off the logits for the next token from a lookup table\n",
    "self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "...\n",
    "self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "...\n",
    "# forward method\n",
    "logits = self.lm_head(x)\n",
    "```\n",
    "\n",
    "Intuitively, we're trying to produce a probability for every dingle token that might come next at every point in time of that Transformer and if we have more and more tokens, we need to produce more and more probabilties. So, every additional token is going to introduce an additional dot product that we have to do in the linear layer that we have to do for this final layer in the Transformer. \n",
    "\n",
    "Why can't the `vocab_size` be very large? \n",
    "\n",
    "1. The embedding table and linear layer are going to grow and become computationally expensive\n",
    "\n",
    "2. A large `vocab_size` would mean more parameters and these additional parameters could be undertrained\n",
    "\n",
    "    * Intuitively, if you have a very large vocabulary size (say, a milllion tokens), then every one of these tokens will come up more and more rarely in the training data because there's a lot more other tokens all over the place. So, we will see fewer and fewer examples of each individual token and you might be worried that the vectors associated with every token will be undertrained as a result because they just don't come up too often and they don't participate in the forward and backward pass.\n",
    "\n",
    "3. In addition, as `vocab_size` increases, the sequences will start shrinking a lot\n",
    "\n",
    "    * This has a nice property where the model will be able to attend to more and more text but you might now be worried that two large chunks are being squished into single tokens. This might cause the model to not have as much \"time to think\" per some number of characters in the text. Basically, we are squishing too mcuh information into a single token and the forward pass of the Transformer is not sufficient to actually process that information appropriately.\n",
    "\n",
    "These are some of the considerations to think about when setting the `vocab_size` which is mostly an empirical hyperparameter. In state-of-the-art architectures today, this is usually in the high 10,000 or 100,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension of `vocab_size` <a id=\"extension\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to take a pre-trained model and extend the `vocab_size`? This is done fairly commonly. For example, when fine-tuning for ChatGPT, a lot more new special tokens get introduced on top of the base model to maintain the metadata and all the structure of conversation objects between a user and an assistant. You might also try to throw in special tokens for using the browser or other tools. It is very temping to add all kinds of special tokens for all kinds of special functionality. \n",
    "\n",
    "So if you want to be adding tokens, that's totally fine. All we have to do is:\n",
    "\n",
    "1. Resize the embedding layer (add rows)\n",
    "\n",
    "2. Initialize these parameters from scratch to be small random numbers and extend the weight inside the final linear layer where we can start making dot products with these associated parameters to bascially calculate the probabilities for these new tokens\n",
    "\n",
    "Both if these are just a resizing operation which is a very mild model surgery and can be done fairly easily and it's quite common. Basically you would freeze the base model, introduce these new parameters and then you only train these new parameters to introduce new tokens into the architecture. You can choose to freeze or train arbitrary parts of it and it's totally up to you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Learning to Compress Prompts with Gist Tokens](https://arxiv.org/pdf/2304.08467.pdf) <a id=\"gist-tokens\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists an entire design space of applications in terms of introducing new tokens into a vocabulary that go way beyong just adding special tokens and special new functionality. One example is the paper on [_Learning to Compress Prompts with Gist Tokens_](https://arxiv.org/pdf/2304.08467.pdf). This is a paper on learning to compress prompts with what they called _gist_ tokens. The rough idea is suppose that you're using language models in a setting that requires very long prompts. These long prompts slow everything down because you have to encode them and then you ahve to use them and then you're attending over them and it's just expensive to have very large prompts. So instead what they did in this paper was they introduced new tokens — basically imagine having a few new tokens and you put them in a sequence — and then trained the model by distillation (entire model is kept frozen and only the representations or embeddings of the new tokens are trained). The new tokens are optimized over such that the behaviour of the language model is identical to the model that has a very long prompt that works for you. SO it's a compression technique of compressing that very long prompt into those few new gist tokens. So you can train this and at test time, you can discard your old prompt and just swap in those tokens and they can sort of \"stand in\" for the very long prompt (that was discarded) and have an almost identical performance. This is one technique and a class of [parameter efficient fune-tuning (PEFT)](https://huggingface.co/docs/peft/index) techniques where most of the model is basically fixed and there's no training of the model weights and there's no training of new parameters like in [_LoRA: Low-Rank Adaptation of Large Languauge Models_](https://arxiv.org/pdf/2106.09685.pdf). The parameters that you're training are now just token embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodality <a id=\"multimodality\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, there's been a lot of momentum in how you could actually construct Transformers that can simulataneously process not just text, but also images, audio, and videos. How do you feed in all these modalities and potentially predict these modalities from a Transformer? Do you have to change the architecture in some fundamental way? What most people are converging towards is that you're not changing the architecture. You stick with the transformer and you just kind of tokenize your input domains and then call it a day and pretend it's just text tokens and doing everything else in an identical manner.\n",
    "\n",
    "**Examples**\n",
    "1. [Taming Transformers for High-Resolution Image Synthesis](https://compvis.github.io/taming-transformers/)\n",
    "2. [Sora](https://openai.com/sora)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisiting and explaining the quirks of LLM tokenization <a id=\"revisiting-and-explaining-the-quirks-of-llm-tokenization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why can't LLM spell words? <a id=\"why-cant-llm-spell-words\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why can't LLM spell or do other spelling related tasks well? \n",
    "\n",
    "So fundamentally this is because, as we saw, these characters are chunked up into tokens and some of these tokens are actually fairly long. If you were to look at GPT-4's vocabulary, one of the longest tokens is \".DefaultCellStyle\" which turns out to be a single individual token. So that's a lot of characters for a single token. Therefore, the suspicion would be that the model should not be very good at tasks related to spelling of the word \".DefaultCellStyle\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Examples:**\n",
    "\n",
    "**User:** `How many letters \"l\" are there in \".DefaultCellStyle\"`\n",
    "\n",
    "**ChatGPT:** `The word \".DefaultCellStyle\" has **three** letters \"l\"`\n",
    "\n",
    "---\n",
    "\n",
    "**User:** `Reverse the string: \".DefaultCellStyle\". Print it as a comma-separated list of characters (no tools)`\n",
    "\n",
    "**ChatGPT:** `\"t\", \"l\", \"u\", \"a\", \"F\", \"t\", \"l\", \"u\", \"a\", \"e\", \"D\", \".\", \"y\",\"t\", \"l\", \"e\", \"S\", \".\"`\n",
    " \n",
    "---\n",
    "\n",
    "**User:** `Reverse the string: \".DefaultCellStyle\". Take the following approach. First print out every single character seaprated by spaces. Then as a step 2 print out all the characters backwards, again separated by spaces. (no tools)`\n",
    "\n",
    "**ChatGPT:** `First, let's print out every single character of the string \".DefaultCellStyle\" separated by spaces:`\n",
    "```python\n",
    ". D e f a u l t C e l l S t y l e\n",
    "```\n",
    "`Now, let's reverse the string and print out all the characters backwards, again separated by spaces:`\n",
    "```python\n",
    "e l y t S l l e C t l u a f e D .\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that ChatGPT was able to get the last task right. It was able to spell out the word letter by letter correctly and successfully reverse the string. But somehow, it can't reverse the word directly. This is most likely because when the word is broken up into individual characters, it is now much easier for the model to \"see\" these individual tokens and successfully reverse them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is LLM worse at non-English languages (e.g. Japanese)? <a id=\"why-is-llm-worse-at-non-english-languages\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not only that the language model sees less non-English data during training of the model parameters but also the tokenizer is not sufficiently trained on non-English data. For example, \"Hello how are you?\" is five tokens and \"안녕하세요. 어떻게 지내세요\" is 15 tokens. This is a three times blow up!\n",
    "\n",
    "\"안녕하세요\" which simply means \"hello\" in Korean ends up being three tokens which is surprising looking at how it's a very common phrase. The English word \"hello\" is only a single token! So basically everything is more bloated and diffuse and that is partly the reason the model works worse on other languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is LLM bad at simple arithmetic? <a id=\"why-is-llm-bad-at-simple-arithmetic\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That has to do with the tokenization of numbers. For example, you will notice that for addition, it is an algorithm that works on the character level. We would add the ones, then the tens, followed by the hundreds and thousands etc. You have to refer to specific parts of these digits. However, these numbers are represented completely arbitrarily based on whatever happened to merge or not merge during the tokenization process.\n",
    "\n",
    "There is an entire [blog post](https://www.beren.io/2023-02-04-Integer-tokenization-is-insane/) about this and it is quite good. The author basically systematically explores the tokenization of numbers. For example for four digit numbers, you can examine whether it is a single token, or tokens that is a 1-3, or a 2-2 or 3-1 combination. So all the different numbers are all the different combinations and you can imagine this is all completely arbitary and the model sometimes sees a token for all four digits, sometimes for three, two or one. This is definitely a headwind, if you will, for language models and is not ideal. \n",
    "\n",
    "**Note:** This could be why at Meta, when Llama 2 was trained, `sentencepiece` was used to split up all the digits. This is partly to improve simple arthmetic performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did GPT-2 have more than necessary trouble coding in Python? <a id=\"why-did-gpt-2-have-more-than-necessary-trouble-coding-in-python\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this is partly a modeling issue on the architecture, the dataset and the strength of the model but it's also partially a tokenization issue. Because as we saw [previously](#example-python) in the simple Python example, the encoding efficiency of the tokenizer for handling spaces in Python is terrible (every single space is an individual token) and this dramatically reduces the context length that the model can attend to. That's almost like a tokenization bug for GPT-2 which was later fixed in GPT-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"? <a id=\"why-did-my-llm-abruptly-halt-when-it-sees-the-string-end-of-text\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Example:**\n",
    "\n",
    "**User:** `Print the string <|endoftext|>`\n",
    "\n",
    "**ChatGPT:** `Could you please specify the string you'd like to print?`\n",
    "\n",
    "**User:** `Echo this exact string back to me: <|endoftext|>`\n",
    "\n",
    "**ChatGPT:** `It seems like there might have been an issue with the input, as I didn't receive a string to echo back. Could you please provide the string again?`\n",
    "\n",
    "**User:** `\"<|endoftext|>\" is the string`\n",
    "\n",
    "**ChatGPT:** `Here's the string you asked to be echoed back: \"\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the example above, it is obvious that something is breaking with respect to then handling of the special token and it is unclear what OpenAI is doing under the hood here, and whether they are potentially parsing the string \"<|endoftext|>\" as an actual token instead of this just being a string. It might be that when `.encode` is called, the `allowed_special` argument is passed and is allowing \"<|endoftext|>\" as a special character in the user prompt. However, the user prompt is sort of a attacker controlled text so you would hope that OpenAI don't really parse or use special tokens from the user's prompt but there's something definitely going wrong here. So your knowledge of these special tokens ends up being in an attack surface potentially and if you'd like to confuse LLMs then you can try to give them some special tokens to see if they are breaking by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is this weird warning I get about a \"trailing whitespace\"? <a id=\"what-is-this-weird-warning-i-get-about-a-trailing-whitespace\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This issue can be illustrated with `gpt-3.5-turbo-instruct` which is a completion model (not a chat model). Imagine giving it a tagline as follows:\n",
    "\n",
    "```\n",
    "Here is a tag line for an ice cream shop:\n",
    "```\n",
    "_\"Scoops of happiness in every cone!\"_\n",
    "\n",
    "That's a great tag line! Now, suppose you add a whitespace in front of your prompt instead of a breakline. You would get a warning as follows:\n",
    "\n",
    "```\n",
    "Warning: Your text ends in a trailing space, which causes worse performance due to how the API splits text into tokens.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happending here? Suppose you found the completion in the training document somewhere on the internet and the LLM trained on this data. As an exmaple:\n",
    "\n",
    "```\n",
    "Here is a tag line for an ice cream shop: Oh yeah\n",
    "```\n",
    "\n",
    "Notice how there is a space after the colon. Spaces are always a prefix to tokens in GPT. Therefore, it is not just an \"Oh\" token but an \" Oh\" token with a whitespace for prefix. Therefore, what's happening above is that when you did not add a whitespace after the colon, the model was able to sample an \" Oh\" token (with a whitespace prefix). However, when you add a whitespace at the end of your prompt, it gets tokenized into its own token which would have otherwise been a part of the tag line. Suddenly, this is an out-of-distribution for the model because the whitespace should have been a part of the next token and the model has seen very very little data of actual whitespace by itself.\n",
    "\n",
    "Going back to our \".DefaultCellStyle\" example from [above](#why-cant-llm-spell-words), it is almost guaranteed that the model has never seen \".DefaultCellSty\" (without the last two letters) in its training set. It is almost definite that it sees the word as a single group. Therefore, if you took \".DefaultCellSty\" as a prompt to the `gpt-3.5-turbo-instruct` completion model and tried to complete from it, you would get a big error:\n",
    "\n",
    "```\n",
    "The model predicted a completion that begins with a stop sequence, resulting in no output. Consider adjusting your prompt or stop sequences.\n",
    "```\n",
    "\n",
    "What likely happened here was that the model immediately emitted the end of text token (it basically predicted ths top sequence immediately) and resulted in no completion and the subsequent warning. We are off the data distribution and the model is predicting totally arbitrary things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why the LLM break if I ask it about \"SolidGoldMagikarp\"? <a id=\"why-the-llm-break-if-i-ask-it-about-solidgoldmagikarp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It comes from this [blog post](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation). What the author did was he went to the token embeddings table and clustered the tokens based on their embedding representation and he noticed that there was this cluster of tokens that looked really strange. So what are these tokens and where did they even come from? What's even \"SolidGoldMagikarp\"? If you ask the LLM about the weird words in this cluster, you would get back a variety of totally broken LLM behaviour. So basically there are a bunch of trigger words and if you ask the model of these trigger words or you just include them in your prompt, the model goes haywire and has all kinds of really strange behaviours. \n",
    "\n",
    "What is happening here?\n",
    "\n",
    "This again comes down to tokenization. What's happening here is that \"SolidGoldMagikarp\" if you actually dig into it is a Reddit user (i.e. `u/SolidGoldMagikarp`). What is thought to have happened is that the tokenization dataset is very different from the training dataset for the actual language model. So in the tokenization dataset, there was potentially a ton of Reddit data where the user \"SolidGoldMagikarp\" was mentioned in the comments. Because the user \"SolidGoldMagikarp\" was a person who posted a lot, this would be a string that occurs many times in the tokenization dataset. Because it appears many times, these tokens would end up getting merged into the single individual token for that single Reddit user. So they would have a dedicated token for that user in the 50,000 vocabulary in GPT-2 that is devoted to that Reddit user. Then what happens is when you train the model, this data from Reddit was not present in the training dataset and therefore, the token \"SolidGoldMagikarp\" never appears and never gets activated. It's initialized at random in the beginning of optimization, then you have forward and backward passes and updates the model and this token is never updated in the embedding table. That row vector never gets sampled and it never gets used so it never gets trained. It's completely untrained and this is like an unallocated memory in a typical binary program written in C. Then at test time, when you invoke this token, then you're basically \"plucking\" out a row in the embedding table that is completely untrained and feeds into the Transformer and creates various undefined behaviours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why should I prefer to use YAML over JSON with LLMs? <a id=\"why-should-i-prefer-to-use-yaml-over-json-with-llms\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "  \"product\": {\n",
    "    \"type\": \"T-Shirt\",\n",
    "    \"price\": 20.0,\n",
    "    \"sizes\": [\"S\", \"M\", \"L\"],\n",
    "    \"reviews\": [\n",
    "      {\n",
    "        \"username\": \"userl\",\n",
    "        \"rating\": 4,\n",
    "        \"created_at\": \"2023-04-19T12:30:002\"\n",
    "      },\n",
    "      { \"username\": \"user2\", \"rating\": 5, \"created_at\": \"2023-05-02T15:00:00Z\" }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "```yaml\n",
    "product:\n",
    "    type: T-Shirt\n",
    "    price: 20.00\n",
    "    sizes:\n",
    "        - S\n",
    "        - M\n",
    "        - L\n",
    "    reviews:\n",
    "        - username: user1\n",
    "          rating: 4\n",
    "          created _at: \"2023-04-19T12:30:002\"\n",
    "        - username: user2\n",
    "          rating: 5\n",
    "          created_at: \"2023-05-02T15:00:00Z\"\n",
    "```\n",
    "\n",
    "Different kinds of formats and different kinds of representations and different languages and so on might be more or less efficient with GPT tokenizers or any tokenizers for that matter. For example, JSON is really dense in tokens and YAML is a lot more efficient in tokens. In the above JSON and YAML formats, JSON has 116 tokens while YAML has 99 tokens — so quite a bit of improvement. \n",
    "\n",
    "In the token economy, users are paying per token in many ways and you're paying in context length and dollar amount for the cost of processing all these kinds of structured data. Therefore, YAMLs is preferred over JSONs. In general the tokenization densities is something that you have to sort of care about and worry about at all times and try to find efficient encoding schemes and spend a lot of time in tokenizer and measure the different token efficiencies of different formats and settings and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
