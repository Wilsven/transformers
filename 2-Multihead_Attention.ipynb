{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Head Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer-architecture-1.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![diagram](images/attention-head-diagram.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sequence_length` is the length of the input sequence. Typically, this would be set to the maximum sequence length so that all of the input vectors are of fixed size. Other input sequences with lengths less than the maximum sequence length are padded.\n",
    "\n",
    "The `batch_size` is going to help with parallel processing and training of the GPU. In this example, we will keep it simple and set the batch size to 1.\n",
    "\n",
    "The `input_dim` is the input vector dimension of every word that goes into the attention unit.\n",
    "\n",
    "The `d_model` is the output vector dimension of every word from the attention unit.\n",
    "\n",
    "We are generating a randomly sampled input `x` because we are not creating the positional encoding in the input phase right now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = 4\n",
    "batch_size = 1\n",
    "input_dim = 512\n",
    "d_model = 512\n",
    "\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n",
    "x.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will be mapping the input from the input dimension to the output dimension multiplied by 3. This is done to create the query, key and value matrices, all concatenated and having all of the attention heads which we will split up later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_layer = nn.Linear(input_dim, 3 * d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 1536])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv_layer(x)\n",
    "qkv.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea what kind of values we are getting, we can plot the distribution of the `qkv` matrix. Since, we randomly sampled from a normal distribution for our input `x`, we should expect the values to be normally distributed.\n",
    "\n",
    "**Note:** Do note that the distribution of values in the `qkv` matrix is going to be very different depending on how we generate the data, the inputs and the positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGxCAYAAABIjE2TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAq7klEQVR4nO3de3xU5Z3H8e8YyJBgMpAgM8ySQExTl3IVsFkilrBAlHItKLC4XBQrlEvNAoJIlWDXpCALqKlY3RaoGLDbNYDFCqEgyAtYuZiitIUVw82QBiHOBIgJhLN/sIwOCZfBCfMk+bxfr/N6Oc95zjm/OYLz9TnPOcdmWZYlAAAAg9wW6gIAAACuREABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAFqOZvNpsmTJ9/y4x4+fFg2m03Lli3ztWVkZMhmswW0n3PnzikjI0Pvv/9+QNtVd6zWrVurf//+Ae3nenJycrR48eJq19lsNmVkZAT1eAAuIaAACJrHHntMO3bsCGibc+fOae7cuQEHlJs51s24VkDZsWOHHnvssRqvAaiPGoS6AAB1R8uWLdWyZcsaPca5c+cUGRl5S451Pf/0T/8U0uMDdRkjKICh1q1bp06dOslutyshIUELFiy4oUsolmXp6aefVsOGDfX666/r5MmTCg8P1zPPPFOl79/+9jfZbDa99NJL19xnYWGhhg0bpqioKDkcDg0fPlxFRUVV+lVX36ZNm5SamqrY2FhFREQoPj5eQ4cO1blz53T48GHdcccdkqS5c+fKZrPJZrNp7Nixfvvbu3evHnzwQTVt2lSJiYlXPdZlubm56tChgxo1aqQ777yzyvdbtmyZbDabDh8+7Nf+/vvvy2az+UZzUlNTtW7dOh05csRX2zePWd0lnk8++USDBg1S06ZN1ahRI3Xq1EnLly+v9jgrV67U7Nmz5Xa7FR0drd69e+vAgQPVfiegvmEEBTDQn/70Jw0aNEjdunXTqlWrVFlZqfnz5+vvf//7NbcrLy/X2LFjtW7dOr3zzjt64IEHJEn9+/fX8uXLNXfuXN1229f/X7J06VKFh4fr4Ycfvuo+y8rK1Lt3bxUWFiorK0vf/e53tW7dOg0fPvy63+Pw4cPq16+f7rvvPv3mN79RkyZN9Pnnn+u9995TRUWFWrRooffee08PPPCAxo0b57tccjm0XDZkyBCNGDFCEyZM0NmzZ695zPz8fKWnpysjI0Mul0tvvvmmnnjiCVVUVGj69OnXrfmbXnnlFT3++OM6dOiQcnNzr9v/wIEDSklJUfPmzfXSSy8pNjZWK1as0NixY/X3v/9dM2bM8Ov/9NNP695779V//ud/yuv1aubMmRowYID++te/KiwsLKBagTrHAmCc5ORky+12W2VlZb42r9drxcTEWFf+tZVkTZo0yTp16pTVvXt36x/+4R+s/Px8vz5r1661JFkbNmzwtV24cMFyu93W0KFDr1nLkiVLLEnWmjVr/Np//OMfW5KspUuX+trmzJnjV9/vf/97S1KVer7p5MmTliRrzpw5VdZd3t+zzz571XXf1KpVK8tms1U5Xp8+fazo6Gjr7NmzlmVZ1tKlSy1JVkFBgV+/zZs3W5KszZs3+9r69etntWrVqtrar6x7xIgRlt1ut44ePerXr2/fvlZkZKT15Zdf+h3nhz/8oV+/3/3ud5Yka8eOHdUeD6hPuMQDGObs2bPatWuXhgwZokaNGvnao6KiNGDAgGq3KSgoULdu3eT1erVz50517NjRb33fvn3lcrm0dOlSX9v69etVWFioRx999Jr1bN68WVFRURo4cKBf+8iRI6/7XTp16qTw8HA9/vjjWr58uT777LPrblOdoUOH3nDftm3bVvn+I0eOlNfr1d69e2/q+Ddq06ZN6tWrl+Li4vzax44dq3PnzlWZ1HvlOe3QoYMk6ciRIzVaJ1AbEFAAw5SUlOjixYtyuVxV1lXXJkkffvihDh48qOHDh1c7cbRBgwYaNWqUcnNz9eWXX0q6NA+jRYsWuv/++69Zz6lTp+R0Om+4lm9KTEzUxo0b1bx5c02aNEmJiYlKTEzUiy++eN1tv6lFixY33Pda5+3UqVMBHTdQp06dqrZWt9td7fFjY2P9PtvtdkmXLqsB9R0BBTBM06ZNZbPZqp2EWl2bJA0fPlw///nPNXv2bP37v/97tX0eeeQRffXVV1q1apVKSkq0du1ajR49+rpzHWJjY6ud+3K1Wq5033336Z133pHH49HOnTvVrVs3paena9WqVTe0vaSAnq1yrfN2ORBcHpkqLy/36/fFF1/c8HGqExsbqxMnTlRpLywslCQ1a9bsW+0fqE8IKIBhGjdurO9///t6++239dVXX/naS0tL9c4771x1u5/97GdavHixnn32Wc2aNavK+jZt2ig5OVlLly5VTk6OysvL9cgjj1y3np49e6q0tFRr1671a8/JyQngW0lhYWFKTk7WL3/5S0nyXW4J9qjB/v379ec//9mvLScnR1FRUercubOkSw90k6R9+/b59bvyO16u70Zr69WrlzZt2uQLJJf99re/VWRkJLclAwHgLh7AQD//+c/1wAMPqE+fPpo2bZoqKys1b948NW7cWKdPn77qdk888YRuv/12Pf744zpz5oxeeuklv9GHRx99VOPHj1dhYaFSUlJ01113XbeW0aNHa9GiRRo9erSef/55JSUl6d1339X69euvu+2rr76qTZs2qV+/foqPj9dXX32l3/zmN5Kk3r17S7o0t6ZVq1Zas2aNevXqpZiYGDVr1swXIgLldrs1cOBAZWRkqEWLFlqxYoXy8vI0b948RUZGSpLuuece3XXXXZo+fbouXLigpk2bKjc3V9u2bauyv/bt2+vtt9/WkiVL1KVLF912223q2rVrtceeM2eO/vCHP6hnz5569tlnFRMTozfffFPr1q3T/Pnz5XA4buo7AfVSqGfpAqje2rVrrQ4dOljh4eFWfHy89Ytf/KLaO1f0/3fxfNPKlSutBg0aWI888ohVWVnpa/d4PFZERIQlyXr99ddvuJbjx49bQ4cOtW6//XYrKirKGjp0qLV9+/br3sWzY8cO60c/+pHVqlUry263W7GxsVaPHj2stWvX+u1/48aN1t13323Z7XZLkjVmzBi//Z08ebJKTVe7i6dfv37W73//e6tt27ZWeHi41bp1a2vhwoVVtj948KCVlpZmRUdHW3fccYc1ZcoUa926dVXu4jl9+rT14IMPWk2aNLFsNpvfMVXN3Ucff/yxNWDAAMvhcFjh4eFWx44d/c6RZX19F89//dd/+bUXFBRUOadAfWWzLMsKSTICELCMjAzNnTtX/LUFUNcxBwUAABiHgAIAAIzDJR4AAGAcRlAAAIBxCCgAAMA4BBQAAGCcWvmgtosXL6qwsFBRUVEBPQIbAACEjmVZKi0tldvt1m23XXuMpFYGlMLCwipvCwUAALXDsWPHqn2x6TfVyoASFRUl6dIXjI6ODnE1AADgRni9XsXFxfl+x6+lVgaUy5d1oqOjCSgAANQyNzI9g0myAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMZpEOoCAJglcUFiqEsIukPTD4W6BAABYgQFAAAYh4ACAACMQ0ABAADGCTigbN26VQMGDJDb7ZbNZtPq1auv2nf8+PGy2WxavHixX3t5ebmmTJmiZs2aqXHjxho4cKCOHz8eaCkAAKCOCjignD17Vh07dlR2dvY1+61evVr/8z//I7fbXWVdenq6cnNztWrVKm3btk1nzpxR//79VVlZGWg5AACgDgr4Lp6+ffuqb9++1+zz+eefa/LkyVq/fr369evnt87j8ejXv/613njjDfXu3VuStGLFCsXFxWnjxo26//77Ay0JAADUMUGfg3Lx4kWNGjVKTz75pNq2bVtl/Z49e3T+/HmlpaX52txut9q1a6ft27dXu8/y8nJ5vV6/BQAA1F1Bfw7KvHnz1KBBA/30pz+tdn1RUZHCw8PVtGlTv3an06mioqJqt8nKytLcuXODXSqAeiLQZ7vw3BQg9II6grJnzx69+OKLWrZsmWw2W0DbWpZ11W1mzZolj8fjW44dOxaMcgEAgKGCGlA++OADFRcXKz4+Xg0aNFCDBg105MgRTZs2Ta1bt5YkuVwuVVRUqKSkxG/b4uJiOZ3Oavdrt9sVHR3ttwAAgLorqAFl1KhR2rdvn/Lz832L2+3Wk08+qfXr10uSunTpooYNGyovL8+33YkTJ/TJJ58oJSUlmOUAAIBaKuA5KGfOnNGnn37q+1xQUKD8/HzFxMQoPj5esbGxfv0bNmwol8ulu+66S5LkcDg0btw4TZs2TbGxsYqJidH06dPVvn173109AACgfgs4oOzevVs9e/b0fZ46daokacyYMVq2bNkN7WPRokVq0KCBhg0bprKyMvXq1UvLli1TWFhYoOUAQNBdnlTLZFkgdGyWZVmhLiJQXq9XDodDHo+H+ShAkNXFtxnfLAIKEFyB/H7zLh4AAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACM0yDUBQAwQ+KCxFCXAAA+jKAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOPwJFkAuIobfbruoemHargSoP5hBAUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYJyAA8rWrVs1YMAAud1u2Ww2rV692rfu/Pnzmjlzptq3b6/GjRvL7XZr9OjRKiws9NtHeXm5pkyZombNmqlx48YaOHCgjh8//q2/DAAAqBsCDihnz55Vx44dlZ2dXWXduXPntHfvXj3zzDPau3ev3n77bR08eFADBw7065eenq7c3FytWrVK27Zt05kzZ9S/f39VVlbe/DcBAAB1hs2yLOumN7bZlJubq8GDB1+1z65du/T9739fR44cUXx8vDwej+644w698cYbGj58uCSpsLBQcXFxevfdd3X//fdf97her1cOh0Mej0fR0dE3Wz6Ab7jRp6aiKp4kC9yYQH6/a3wOisfjkc1mU5MmTSRJe/bs0fnz55WWlubr43a71a5dO23fvr3afZSXl8vr9fotAACg7qrRgPLVV1/pqaee0siRI31JqaioSOHh4WratKlfX6fTqaKiomr3k5WVJYfD4Vvi4uJqsmwAABBiNRZQzp8/rxEjRujixYt65ZVXrtvfsizZbLZq182aNUsej8e3HDt2LNjlAgAAg9RIQDl//ryGDRumgoIC5eXl+V1ncrlcqqioUElJid82xcXFcjqd1e7PbrcrOjrabwEAAHVX0APK5XDyv//7v9q4caNiY2P91nfp0kUNGzZUXl6er+3EiRP65JNPlJKSEuxyAABALdQg0A3OnDmjTz/91Pe5oKBA+fn5iomJkdvt1oMPPqi9e/fqD3/4gyorK33zSmJiYhQeHi6Hw6Fx48Zp2rRpio2NVUxMjKZPn6727durd+/ewftmAACg1go4oOzevVs9e/b0fZ46daokacyYMcrIyNDatWslSZ06dfLbbvPmzUpNTZUkLVq0SA0aNNCwYcNUVlamXr16admyZQoLC7vJrwEAAOqSb/UclFDhOShA8PEclJvHc1CAG2PUc1AAAAACRUABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYJ+F08AAB/V74mgEffA98eIygAAMA4BBQAAGAcLvEA9RRvLwZgMkZQAACAcRhBAeoZRk4A1AaMoAAAAOMQUAAAgHG4xAPUE1zaAVCbMIICAACMQ0ABgCBLXJDIiBXwLRFQAACAcQgoAADAOAQUAABgHAIKAAAwDrcZA3UUkzQB1GaMoAAAAOMwggLUcoyUAKiLGEEBAADGIaAAAADjEFAAAIBxCCgAAMA4TJIFaikmxwKoyxhBAQAAxiGgAAAA43CJB6gluKQDoD5hBAUAABiHgAIAAIwTcEDZunWrBgwYILfbLZvNptWrV/uttyxLGRkZcrvdioiIUGpqqvbv3+/Xp7y8XFOmTFGzZs3UuHFjDRw4UMePH/9WXwQAANQdAQeUs2fPqmPHjsrOzq52/fz587Vw4UJlZ2dr165dcrlc6tOnj0pLS3190tPTlZubq1WrVmnbtm06c+aM+vfvr8rKypv/JgAAoM4IeJJs37591bdv32rXWZalxYsXa/bs2RoyZIgkafny5XI6ncrJydH48ePl8Xj061//Wm+88YZ69+4tSVqxYoXi4uK0ceNG3X///d/i6wAAgLogqHNQCgoKVFRUpLS0NF+b3W5Xjx49tH37dknSnj17dP78eb8+brdb7dq18/W5Unl5ubxer98CAADqrqAGlKKiIkmS0+n0a3c6nb51RUVFCg8PV9OmTa/a50pZWVlyOBy+JS4uLphlAwAAw9TIXTw2m83vs2VZVdqudK0+s2bNksfj8S3Hjh0LWq0AAMA8QQ0oLpdLkqqMhBQXF/tGVVwulyoqKlRSUnLVPley2+2Kjo72WwAAQN0V1ICSkJAgl8ulvLw8X1tFRYW2bNmilJQUSVKXLl3UsGFDvz4nTpzQJ5984usDAADqt4Dv4jlz5ow+/fRT3+eCggLl5+crJiZG8fHxSk9PV2ZmppKSkpSUlKTMzExFRkZq5MiRkiSHw6Fx48Zp2rRpio2NVUxMjKZPn6727dv77uoBAAD1W8ABZffu3erZs6fv89SpUyVJY8aM0bJlyzRjxgyVlZVp4sSJKikpUXJysjZs2KCoqCjfNosWLVKDBg00bNgwlZWVqVevXlq2bJnCwsKC8JUAAEBtZ7Msywp1EYHyer1yOBzyeDzMR0G9wcsCa59D0w+FugTAKIH8fvMuHgAAYBwCCgAAMA4BBQAAGCfgSbIAgBtz5bwh5qQAN44RFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjNAh1AQBQXyQuSPT7fGj6oRBVApiPERQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYByeJAsAIXLlk2WvxJNmUZ8xggIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxgl6QLlw4YJ+9rOfKSEhQREREbrzzjv13HPP6eLFi74+lmUpIyNDbrdbERERSk1N1f79+4NdCgAAqKWCHlDmzZunV199VdnZ2frrX/+q+fPn64UXXtDLL7/s6zN//nwtXLhQ2dnZ2rVrl1wul/r06aPS0tJglwMAAGqhoL8scMeOHRo0aJD69esnSWrdurVWrlyp3bt3S7o0erJ48WLNnj1bQ4YMkSQtX75cTqdTOTk5Gj9+fJV9lpeXq7y83PfZ6/UGu2wAAGCQoI+gdO/eXX/605908OBBSdKf//xnbdu2TT/84Q8lSQUFBSoqKlJaWppvG7vdrh49emj79u3V7jMrK0sOh8O3xMXFBbtsAABgkKCPoMycOVMej0f/+I//qLCwMFVWVur555/Xv/zLv0iSioqKJElOp9NvO6fTqSNHjlS7z1mzZmnq1Km+z16vl5ACAEAdFvSA8tZbb2nFihXKyclR27ZtlZ+fr/T0dLndbo0ZM8bXz2az+W1nWVaVtsvsdrvsdnuwSwUAAIYKekB58skn9dRTT2nEiBGSpPbt2+vIkSPKysrSmDFj5HK5JF0aSWnRooVvu+Li4iqjKgAAoH4K+hyUc+fO6bbb/HcbFhbmu804ISFBLpdLeXl5vvUVFRXasmWLUlJSgl0OAACohYI+gjJgwAA9//zzio+PV9u2bfXRRx9p4cKFevTRRyVdurSTnp6uzMxMJSUlKSkpSZmZmYqMjNTIkSODXQ4AAKiFgh5QXn75ZT3zzDOaOHGiiouL5Xa7NX78eD377LO+PjNmzFBZWZkmTpyokpISJScna8OGDYqKigp2OQAAoBayWZZlhbqIQHm9XjkcDnk8HkVHR4e6HOCWSFyQGOoScIsdmn4o1CUAQRXI73fQR1AAAMFxZSglsKA+4WWBAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADG4TZjwDA87wQAGEEBAAAGIqAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHgAIAtUTigkSek4N6g4ACAACMw5NkAUPwf8YA8DVGUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA6TZIEQY3IsAFTFCAoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHG4zRioIdw+jJpy+c/WoemHQlwJUHMYQQEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjFMjAeXzzz/Xv/7rvyo2NlaRkZHq1KmT9uzZ41tvWZYyMjLkdrsVERGh1NRU7d+/vyZKAQAAtVDQA0pJSYnuvfdeNWzYUH/84x/1l7/8Rf/xH/+hJk2a+PrMnz9fCxcuVHZ2tnbt2iWXy6U+ffqotLQ02OUAAIBaKOiPup83b57i4uK0dOlSX1vr1q19/2xZlhYvXqzZs2dryJAhkqTly5fL6XQqJydH48ePD3ZJAACglgn6CMratWvVtWtXPfTQQ2revLnuvvtuvf766771BQUFKioqUlpamq/NbrerR48e2r59e7X7LC8vl9fr9VsAAEDdFfSA8tlnn2nJkiVKSkrS+vXrNWHCBP30pz/Vb3/7W0lSUVGRJMnpdPpt53Q6feuulJWVJYfD4Vvi4uKCXTYAADBI0APKxYsX1blzZ2VmZuruu+/W+PHj9eMf/1hLlizx62ez2fw+W5ZVpe2yWbNmyePx+JZjx44Fu2wAAGCQoAeUFi1a6Hvf+55fW5s2bXT06FFJksvlkqQqoyXFxcVVRlUus9vtio6O9lsAAEDdFfSAcu+99+rAgQN+bQcPHlSrVq0kSQkJCXK5XMrLy/Otr6io0JYtW5SSkhLscgAAQC0U9Lt4/u3f/k0pKSnKzMzUsGHD9OGHH+q1117Ta6+9JunSpZ309HRlZmYqKSlJSUlJyszMVGRkpEaOHBnscgAAQC0U9IByzz33KDc3V7NmzdJzzz2nhIQELV68WA8//LCvz4wZM1RWVqaJEyeqpKREycnJ2rBhg6KiooJdDgAAqIVslmVZoS4iUF6vVw6HQx6Ph/koMFbigsRQl4A67tD0Q6EuAQhIIL/fvIsHAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGCcoD+oDQBwa1ztWTs8HwV1ASMoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHlwUC39LVXtgGALh5jKAAAADjEFAAAIBxCCgAAMA4BBQAAGAcJskCQB1zvYnbh6YfukWVADePERQAAGAcAgoAADAOAQUAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjFPjASUrK0s2m03p6em+NsuylJGRIbfbrYiICKWmpmr//v01XQoAAKglajSg7Nq1S6+99po6dOjg1z5//nwtXLhQ2dnZ2rVrl1wul/r06aPS0tKaLAcAANQSNRZQzpw5o4cfflivv/66mjZt6mu3LEuLFy/W7NmzNWTIELVr107Lly/XuXPnlJOTU1PlAACAWqTGAsqkSZPUr18/9e7d26+9oKBARUVFSktL87XZ7Xb16NFD27dvr3Zf5eXl8nq9fgsAAKi7GtTETletWqW9e/dq165dVdYVFRVJkpxOp1+70+nUkSNHqt1fVlaW5s6dG/xCAQCAkYI+gnLs2DE98cQTWrFihRo1anTVfjabze+zZVlV2i6bNWuWPB6Pbzl27FhQawYAAGYJ+gjKnj17VFxcrC5duvjaKisrtXXrVmVnZ+vAgQOSLo2ktGjRwtenuLi4yqjKZXa7XXa7PdilAgAAQwV9BKVXr176+OOPlZ+f71u6du2qhx9+WPn5+brzzjvlcrmUl5fn26aiokJbtmxRSkpKsMsBAAC1UNBHUKKiotSuXTu/tsaNGys2NtbXnp6erszMTCUlJSkpKUmZmZmKjIzUyJEjg10OAACohWpkkuz1zJgxQ2VlZZo4caJKSkqUnJysDRs2KCoqKhTlAAAAw9gsy7JCXUSgvF6vHA6HPB6PoqOjQ10O6rnEBYmhLgEIyKHph0JdAuqpQH6/QzKCAtQFBBMAqDm8LBAAABiHgAIAAIxDQAEAAMYhoAAAAOMQUAAAgHEIKAAAwDgEFAAAYBwCCgAAMA4BBQAAGIeAAgAAjENAAQAAxiGgAAAA4/CyQCBAvCQQtd3lP8O81RgmYwQFAAAYh4ACAACMQ0ABAADGYQ4KcB3MOUFdxVwUmIwRFAAAYBwCCgAAMA6XeICr4NIO6osr/6xzyQcmYAQFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcAgoAADAOAQUAABiHJ8kCAPzwZFmYgBEUAABgHEZQgP/Hu3cAwByMoAAAAOMQUAAAgHG4xAMAuKbrXf5kEi1qAiMoAADAOAQUAABgnKAHlKysLN1zzz2KiopS8+bNNXjwYB04cMCvj2VZysjIkNvtVkREhFJTU7V///5glwIAAGqpoAeULVu2aNKkSdq5c6fy8vJ04cIFpaWl6ezZs74+8+fP18KFC5Wdna1du3bJ5XKpT58+Ki0tDXY5AACgFrJZlmXV5AFOnjyp5s2ba8uWLfrBD34gy7LkdruVnp6umTNnSpLKy8vldDo1b948jR8//rr79Hq9cjgc8ng8io6OrsnyUQ/w/BPg22GSLG5UIL/fNT4HxePxSJJiYmIkSQUFBSoqKlJaWpqvj91uV48ePbR9+/Zq91FeXi6v1+u3AACAuqtGA4plWZo6daq6d++udu3aSZKKiookSU6n06+v0+n0rbtSVlaWHA6Hb4mLi6vJsgEAQIjVaECZPHmy9u3bp5UrV1ZZZ7PZ/D5bllWl7bJZs2bJ4/H4lmPHjtVIvQAAwAw19qC2KVOmaO3atdq6datatmzpa3e5XJIujaS0aNHC115cXFxlVOUyu90uu91eU6UCAADDBH0ExbIsTZ48WW+//bY2bdqkhIQEv/UJCQlyuVzKy8vztVVUVGjLli1KSUkJdjnAVSUuSGSCLAAYKugjKJMmTVJOTo7WrFmjqKgo37wSh8OhiIgI2Ww2paenKzMzU0lJSUpKSlJmZqYiIyM1cuTIYJcDAABqoaAHlCVLlkiSUlNT/dqXLl2qsWPHSpJmzJihsrIyTZw4USUlJUpOTtaGDRsUFRUV7HIAH0ZLAKD2qPHnoNQEnoOCm0FAAWoGz0HBjTLqOSgAAACBIqAAAADjEFAAAIBxauw5KACA+uHK+V3MSUEwMIICAACMQ0ABAADGIaAAAADjEFAAAIBxCCgAAMA4BBQAAGAcbjNGnccj7oFbK9C/c9yWjOowggIAAIxDQAEAAMYhoAAAAOMQUAAAgHGYJIs6i8mxAFB7MYICAACMQ0ABAADG4RIPai0u4QBA3cUICgAAMA4BBQAAGIeAAgAAjENAAQAAxmGSLGodJscCQN3HCAoAADAOIygwHiMmAFD/MIICAACMQ0ABAADGIaAAAADjEFAAAIBxmCQL4zApFgDACAoAADAOIygwBiMnQP10o3/3D00/VMOVwCSMoAAAAOMQUAAAgHG4xIOQ4ZIOgEBc778ZXAKqWxhBAQAAxmEEBQBQJwRrVJaRGDOEdATllVdeUUJCgho1aqQuXbrogw8+CGU5AADAECELKG+99ZbS09M1e/ZsffTRR7rvvvvUt29fHT16NFQlAQAAQ9gsy7JCceDk5GR17txZS5Ys8bW1adNGgwcPVlZW1jW39Xq9cjgc8ng8io6OrulS6w0mrQJAVVzyCZ5Afr9DMgeloqJCe/bs0VNPPeXXnpaWpu3bt1fpX15ervLyct9nj8cj6dIXRfBc/OpiqEsAAOPwWxM8l8/ljYyNhCSgfPHFF6qsrJTT6fRrdzqdKioqqtI/KytLc+fOrdIeFxdXYzUCACBJjmccoS6hziktLZXDce3zGtK7eGw2m99ny7KqtEnSrFmzNHXqVN/nixcv6vTp04qNja22f23h9XoVFxenY8eO1etLVZyHr3EuLuE8fI1z8TXOxSW1+TxYlqXS0lK53e7r9g1JQGnWrJnCwsKqjJYUFxdXGVWRJLvdLrvd7tfWpEmTmizxloqOjq51f8hqAufha5yLSzgPX+NcfI1zcUltPQ/XGzm5LCR38YSHh6tLly7Ky8vza8/Ly1NKSkooSgIAAAYJ2SWeqVOnatSoUeratau6deum1157TUePHtWECRNCVRIAADBEyALK8OHDderUKT333HM6ceKE2rVrp3fffVetWrUKVUm3nN1u15w5c6pcvqpvOA9f41xcwnn4Gufia5yLS+rLeQjZc1AAAACuhpcFAgAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgHFEAMHDlR8fLwaNWqkFi1aaNSoUSosLAx1WbfU4cOHNW7cOCUkJCgiIkKJiYmaM2eOKioqQl1aSDz//PNKSUlRZGRknXpy8o145ZVXlJCQoEaNGqlLly764IMPQl3SLbd161YNGDBAbrdbNptNq1evDnVJIZGVlaV77rlHUVFRat68uQYPHqwDBw6EuqyQWLJkiTp06OB7gmy3bt30xz/+MdRl1RgCiiF69uyp3/3udzpw4ID++7//W4cOHdKDDz4Y6rJuqb/97W+6ePGifvWrX2n//v1atGiRXn31VT399NOhLi0kKioq9NBDD+knP/lJqEu5pd566y2lp6dr9uzZ+uijj3Tfffepb9++Onr0aKhLu6XOnj2rjh07Kjs7O9SlhNSWLVs0adIk7dy5U3l5ebpw4YLS0tJ09uzZUJd2y7Vs2VK/+MUvtHv3bu3evVv//M//rEGDBmn//v2hLq1G8BwUQ61du1aDBw9WeXm5GjZsGOpyQuaFF17QkiVL9Nlnn4W6lJBZtmyZ0tPT9eWXX4a6lFsiOTlZnTt31pIlS3xtbdq00eDBg5WVlRXCykLHZrMpNzdXgwcPDnUpIXfy5Ek1b95cW7Zs0Q9+8INQlxNyMTExeuGFFzRu3LhQlxJ0jKAY6PTp03rzzTeVkpJSr8OJJHk8HsXExIS6DNwiFRUV2rNnj9LS0vza09LStH379hBVBZN4PB5Jqvf/XaisrNSqVat09uxZdevWLdTl1AgCikFmzpypxo0bKzY2VkePHtWaNWtCXVJIHTp0SC+//DLvZ6pHvvjiC1VWVlZ5q7nT6azy9nPUP5ZlaerUqerevbvatWsX6nJC4uOPP9btt98uu92uCRMmKDc3V9/73vdCXVaNIKDUoIyMDNlstmsuu3fv9vV/8skn9dFHH2nDhg0KCwvT6NGjVReuwAV6HiSpsLBQDzzwgB566CE99thjIao8+G7mXNRHNpvN77NlWVXaUP9MnjxZ+/bt08qVK0NdSsjcddddys/P186dO/WTn/xEY8aM0V/+8pdQl1UjQvaywPpg8uTJGjFixDX7tG7d2vfPzZo1U7NmzfTd735Xbdq0UVxcnHbu3Fnrh+8CPQ+FhYXq2bOn7y3XdUmg56K+adasmcLCwqqMlhQXF1cZVUH9MmXKFK1du1Zbt25Vy5YtQ11OyISHh+s73/mOJKlr167atWuXXnzxRf3qV78KcWXBR0CpQZcDx824PHJSXl4ezJJCIpDz8Pnnn6tnz57q0qWLli5dqttuq1uDfN/mz0R9EB4eri5duigvL08/+tGPfO15eXkaNGhQCCtDqFiWpSlTpig3N1fvv/++EhISQl2SUSzLqhO/E9UhoBjgww8/1Icffqju3buradOm+uyzz/Tss88qMTGx1o+eBKKwsFCpqamKj4/XggULdPLkSd86l8sVwspC4+jRozp9+rSOHj2qyspK5efnS5K+853v6Pbbbw9tcTVo6tSpGjVqlLp27eobRTt69Gi9m4t05swZffrpp77PBQUFys/PV0xMjOLj40NY2a01adIk5eTkaM2aNYqKivKNrjkcDkVERIS4ulvr6aefVt++fRUXF6fS0lKtWrVK77//vt57771Ql1YzLITcvn37rJ49e1oxMTGW3W63WrdubU2YMME6fvx4qEu7pZYuXWpJqnapj8aMGVPtudi8eXOoS6txv/zlL61WrVpZ4eHhVufOna0tW7aEuqRbbvPmzdX++x8zZkyoS7ulrvbfhKVLl4a6tFvu0Ucf9f29uOOOO6xevXpZGzZsCHVZNYbnoAAAAOPUrQv8AACgTiCgAAAA4xBQAACAcQgoAADAOAQUAABgHAIKAAAwDgEFAAAYh4ACAACMQ0ABAADGIaAAAADjEFAAAIBx/g9j1pZUj6AHFgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_val = torch.histc(qkv, bins=200, min=-3, max=3)\n",
    "x_val = np.arange(-1, 1, 0.01) * 3\n",
    "plt.bar(x_val, y_val, align=\"center\", color=[\"forestgreen\"])\n",
    "plt.title(\"qkv distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we reshape our `qkv` matrix to break down the last dimension of 1536 into a product of `num_heads` and 3 $\\times$ the `head_dim`.\n",
    "\n",
    "`num_heads` refers to the number of attention heads that will make up our attention unit.\n",
    "\n",
    "`head_dim` refers to the number of dimensions per attention head.\n",
    "\n",
    "**Note:** We multiply the `head_dim` by 3 because the `qkv` matrix is a combination of the query, key and value matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 8, 192])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_heads = 8\n",
    "head_dim = d_model // num_heads  # 64\n",
    "qkv = qkv.reshape(batch_size, sequence_length, num_heads, 3 * head_dim)\n",
    "qkv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 192])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv = qkv.permute(0, 2, 1, 3)  # [batch_size, num_heads,sequence_length, 3 * head_dim]\n",
    "qkv.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform the above permutation because it is easier to perform parallel operations on the last two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]),\n",
       " torch.Size([1, 8, 4, 64]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v = qkv.chunk(3, dim=-1)\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention for Multiple Heads"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single head:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "\\text{self attention} = \\text{softmax}(\\frac{Q·K^{T}}{\\sqrt{d_{k}}} + M) \\nonumber\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "\\text{new } V = \\text{self attention}·V \\nonumber\n",
    " \n",
    "\\end{align}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You have to use the `torch.transpose()` function because `k` is a matrix (more specifically, a 4-dimensional tensor) and not simply a 2-dimensional matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = q.size()[-1]  # 64\n",
    "scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The ordering does not matter\n",
    "(k.transpose(-1, -2) == k.transpose(-2, -1)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf],\n",
       "        [0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.full(scaled.size(), float(\"-inf\"))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask[0][1]  # Mask for input to a single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1473,    -inf,    -inf,    -inf],\n",
       "        [ 0.2594,  0.1353,    -inf,    -inf],\n",
       "        [-0.5262, -0.2408,  0.2090,    -inf],\n",
       "        [ 0.4084,  0.0936,  0.3839,  0.4296]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(scaled + mask)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled += mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = F.softmax(scaled, dim=-1)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5310, 0.4690, 0.0000, 0.0000],\n",
       "        [0.2264, 0.3012, 0.4723, 0.0000],\n",
       "        [0.2683, 0.1958, 0.2618, 0.2740]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.matmul(attention, v)\n",
    "values.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function\n",
    "\n",
    "The `scaled_dot_product` below is the same one as the [previous section](1-Self_Attention_for_Transformers_Neural_Network.ipynb) except for the `k.transpose(-2, -1)` where now, we are transposing a higher dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, attention = scaled_dot_product(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2273, 0.3466, 0.2344, 0.1917],\n",
       "        [0.2949, 0.2605, 0.1682, 0.2764],\n",
       "        [0.1710, 0.2274, 0.3566, 0.2450],\n",
       "        [0.2683, 0.1958, 0.2618, 0.2740]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, attention = scaled_dot_product(q, k, v, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 4])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5310, 0.4690, 0.0000, 0.0000],\n",
       "        [0.2264, 0.3012, 0.4723, 0.0000],\n",
       "        [0.2683, 0.1958, 0.2618, 0.2740]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention[0][1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the value matrix for every single head (i.e. 8) for every single word (i.e. 4) which are 64-dimensional vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 4, 64])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we concatenate all of the heads together. For 8 heads, we are now going to have 512-dimensional vectors (because 8 x 64) which is exactly the input dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = values.reshape(batch_size, sequence_length, num_heads * head_dim)\n",
    "values.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these heads to communicate with each another on the information they've learned, we are going to pass the matrix through a linear layer which is simply a feed forward layer of 512 input dimension and 512 output dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = nn.Linear(d_model, d_model)\n",
    "out = linear_layer(values)\n",
    "out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this output matrix is going to be much more context aware than the input matrix was."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled += mask\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(input_dim, 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, sequence_length, _ = x.size()\n",
    "        # [batch_size, sequence_length, input_dim]\n",
    "        print(f\"x.size(): {x.size()}\")\n",
    "\n",
    "        qkv = self.qkv_layer(x)\n",
    "        # [batch_size, sequence_length, 3 * d_model]\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        qkv = qkv.reshape(\n",
    "            batch_size, sequence_length, self.num_heads, 3 * self.head_dim\n",
    "        )\n",
    "        # [batch_size, sequence_length, num_heads, 3 * head_dim] where 3 * num_heads * head_dim = 3 * d_model\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        # [batch_size, num_heads, sequence_length, 3 * head_dim]\n",
    "        print(f\"qkv.size(): {qkv.size()}\")\n",
    "\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        # Each are [batch_size, num_heads, sequence_length, head_dim]\n",
    "        print(f\"q.size(): {q.size()}, k.size(): {k.size()}, v.size(): {v.size()}\")\n",
    "\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        # values.size() -> [batch_size, num_heads, sequence_length, head_dim], attention.size() -> [batch_size, num_heads, sequence_length, sequence_length]\n",
    "        print(f\"values.size(): {values.size()}, attention.size(): {attention.size()}\")\n",
    "\n",
    "        values = values.reshape(\n",
    "            batch_size, sequence_length, self.num_heads * self.head_dim\n",
    "        )\n",
    "        # [batch_size, sequence_length, num_heads * head_dim]\n",
    "        print(f\"values.size(): {values.size()}\")\n",
    "\n",
    "        out = self.linear_layer(values)\n",
    "        # [batch_size, sequence_length, num_heads * head_dim]\n",
    "        print(f\"out.size(): {out.size()}\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size(): torch.Size([30, 5, 1024])\n",
      "qkv.size(): torch.Size([30, 5, 1536])\n",
      "qkv.size(): torch.Size([30, 5, 8, 192])\n",
      "qkv.size(): torch.Size([30, 8, 5, 192])\n",
      "q.size(): torch.Size([30, 8, 5, 64]), k.size(): torch.Size([30, 8, 5, 64]), v.size(): torch.Size([30, 8, 5, 64])\n",
      "values.size(): torch.Size([30, 8, 5, 64]), attention.size(): torch.Size([30, 8, 5, 5])\n",
      "values.size(): torch.Size([30, 5, 512])\n",
      "out.size(): torch.Size([30, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 1024\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "\n",
    "batch_size = 30\n",
    "sequence_length = 5\n",
    "x = torch.randn((batch_size, sequence_length, input_dim))\n",
    "\n",
    "model = MultiHeadAttention(input_dim, d_model, num_heads)\n",
    "out = model.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
