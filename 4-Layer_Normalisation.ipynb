{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Normalisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer-architecture-4.png\" width=\"500\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Neural Network Architecture Overview (Continued)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the Transformer architecture overview section in the [positional encoding notebook](3-Positional_Encoding_in_Transformer_Neural_Network.ipynb), we will be taking a quick walkthrough of the architecture once again before we discuss about layer normalisation.\n",
    "\n",
    "Let's say we have an input sequence that is, *My name is John* and we want to translate this from English to French. We first pad the input sequence to the maximum seqeunce length. Each word is then represented as a one-hot encoded vector.\n",
    "\n",
    "**Note:** Technically, it would not be the words that represent the one-hot encoded vector but word pieces called byte pair encodings but for the sake of simplicity, we will consider them as one-hot encoded word vectors.\n",
    "\n",
    "Because these are one-hot encoded vectors, they are going to be the same length as the vocabulary size — that is all possible words that could possibly occur. We then transform these one-hot encoded vectors into $512$-dimensional word vectors to form matrix $X$. Because all words are passed into the Transformer in parallel, there is no sense of ordering. However, English sentences have words that are ordered specifically. So, we pass in some [positional encoding](3-Positional_Encoding_in_Transformer_Neural_Network.ipynb) to encode orders.\n",
    "\n",
    "We then add the input to the encoding to get the positionally encoded vectors to form matrix $X^{1}$. From here, the multi-head attention unit is kicked off where each positionally encoded vector is now split up into three vectors of query, key and value — each of these are $512-dimensional vectors. So, we are going to end up with 3 $\\times$ the maximum sequence length — it is basically 3 three $\\times$ the number of words in the input sequence. \n",
    "\n",
    "We now split each of these query, key and value vectors into 8 parts and each part (highlighted in yellow) is going to be a vector for one attention head — there are 8 attention heads in the main paper. Each of these heads are then passed into an attention block, $\\text{ATTN}$. An attention block is basically going to multiply their query and key vectors, apply scaling and masking (only for decoder), to form the attention blocks, $a_{i}$ which have the size of maximum sequence length $\\times$ maximum sequence length. These attention blocks, $a_{i}$ tells us exactly how much attention each word should pay to the other words.\n",
    "\n",
    "We then multiply every attention blocks by every head's value vector. In the end, we will get 8 individual vecors of size maximum sequence length $\\times$ 64 each. We concatenate all of these 8 vectors and the final output size will be maximum sequence length $\\times$ $512$ (which is $64 \\times 8 \\text{ heads} = $512$ \\text{ output dimensions}$).\n",
    "\n",
    "This output matrix will be just before the normalisation layer. The normalisation takes in this output layer as well as a residual/skip connection of the matrix after positional encoding. These residual connections are done to ensure that there is a stronger information signal that flows through deep networks. This is required because as we keep back propagating, the gradient updates become zero and the model stops learning. This issue is famously known as vanishing gradients. Therefore, to prevent that, we induce stronger signals from the input in different parts of the network.\n",
    "\n",
    "We then multiply every attention blocks by every head's value vector. In the end, we will get 8 individual vecors of size maximum sequence length $\\times$ 64 each. We concatenate all of these 8 vectors and the final output size will be maximum sequence length $\\times$ $512$ (which is $64 \\times 8 \\text{ heads} = $512$ \\text{ output dimensions}$)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![diagram](images/layer-normalisation.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Layer Normalisation? Why perform Layer Normalisation?\n",
    "\n",
    "Activations of neurons will be a wide range of positive and negative values. Normalisation encapsulates these values within a much smaller range and typically centres around zero. What this allows for is much more stable training during the back propagation phase and when we perform a gradient update step, we are taking much more even and consistent steps — so it is now easier to learn and hence, faster to train till the model gets to the optimal parameter values.\n",
    "\n",
    "Layer normalisation is the strategy in which we apply normalisation to a neural network. In this case, we are going to ensure the activation values of every neuron in every layer is normalised such that all the activation values in a layer will be centered with unit variance (i.e. centered at $0$ and standard deviation of $1$)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![diagram](images/formula.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand layer normalisation in more detail, let's say we have X, Y, Z and O are the activation vectors for each one of the layers as shown in the image above. In a typical neural network fashion, we'd apply some activation, $f$ to the weights, $W$ $\\times$ some vector, $X$ and plus a bias, $b$ (see first equation in image above). This is without any kind of normalisation.\n",
    "\n",
    "To perform normalisation, we'd substract the mean, $\\mu$ of the activation values divided by the standard deviation, $\\sigma$ of the activation values to the output produced from the activation function above. We would also add learnable parameters, $\\gamma$ and $\\beta$ (see second equation in image above).\n",
    "\n",
    "As we keep getting more and more inputs to the network above over time, and we keep performing the back propagation step, the learnable parameters, $\\gamma$ and $\\beta$ are going to change and be learned in order to optimize the objective of the loss function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to perform Layer Normalisation?\n",
    "\n",
    "Let's say we have two input vectors:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "\\begin{bmatrix}\n",
    "0.2 & 0.1 & 0.3\\\\\n",
    "0.5 & 0.1 & 0.1 \n",
    "\\end{bmatrix} \\rightarrow \n",
    "\\text{2 words and 3 dimensions}\n",
    "\n",
    "\\nonumber\n",
    " \n",
    "\\end{align}\n",
    "\n",
    "Now, we want to perform some normalisation — specifically, layer normalisation for the matrix above. To do this, we compute the mean and standard deviation across the layer:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "\\mu_{11} = \\frac{1}{3}[0.2 + 0.1 + 0.3] = 0.2 \\nonumber\n",
    "\n",
    "\\newline\\nonumber\n",
    "\\newline\\nonumber\n",
    " \n",
    "\\mu_{21} = \\frac{1}{3}[0.5 + 0.1 + 0.1] = 0.233 \\nonumber\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "We can now use these $\\mu$ values to compute the standard deviations:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "\\sigma_{11} = \\sqrt{\\frac{1}{3}[(0.2 - 0.2)^{2} +(0.1 - 0.2)^{2} + (0.3 - 0.2)^{2}]} = \\sqrt{\\frac{1}{3}[0.0 + 0.01 + 0.01]} = 0.08164 \\nonumber\n",
    "\n",
    "\\newline\\nonumber\n",
    "\\newline\\nonumber\n",
    " \n",
    "\\sigma_{21} = \\sqrt{\\frac{1}{3}[(0.5 - 0.233)^{2} +(0.1 - 0.233)^{2} + (0.1 - 0.233)^{2}]} = \\sqrt{\\frac{1}{3}[0.071289 + 0.017689 + 0.017689]} = 0.1885 \\nonumber \\nonumber\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "Now, we have the matrices for the means and standard deviations:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "\\mu = \\begin{bmatrix}\n",
    "\\mu_{11}\\\\\n",
    "\\mu_{21}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.2\\\\\n",
    "0.233 \n",
    "\\end{bmatrix} \\nonumber\n",
    "\n",
    "\\newline\\nonumber\n",
    "\\newline\\nonumber\n",
    " \n",
    "\\sigma = \\begin{bmatrix}\n",
    "\\sigma_{11}\\\\\n",
    "\\sigma_{21}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.08164\\\\\n",
    "0.1885\n",
    "\\end{bmatrix} \\nonumber\n",
    "\n",
    "\\end{align}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can substract the mean and divide by the standard deviation:\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "Y = \\frac{X - \\mu}{\\sigma} = \\begin{bmatrix}\n",
    "\\frac{0.2 - 0.2}{0.08164} & \\frac{0.1 - 0.2}{0.08164} & \\frac{0.3 - 0.2}{0.08164}\\\\\n",
    "\\frac{0.5 - 0.233}{0.1885} & \\frac{0.1 - 0.233}{0.1885} & \\frac{0.1 - 0.233}{0.1885}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "0.0 & -1.2248 & 1.2248\\\\\n",
    "1.414 & -0.707 & -0.707\n",
    "\\end{bmatrix} \\nonumber\n",
    "\n",
    "\\end{align}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\n",
    "\\text{out} = \\gamma · Y + \\beta \\nonumber\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "\\gamma, \\beta \\isin \\mathbb{R}^{2} \\nonumber\n",
    "\n",
    "\\end{align}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you'll notice is if $\\gamma$ is set to $1$ and $\\beta$ is set to $0$, the $\\text{out}$ will be same as $Y$. You will also notice that for every single one of those normalised layers, the mean, $\\mu$ is $0$ and standard deviation, $\\sigma$ is close to $1$. Therefore, these values are much more tractable and it becomes much more stable during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice here that a batch dimension, `B` have been added to the same exact input as our example above. In practice, during training, we would typically have a batch dimension so that it helps parallelise training and training just becomes faster.\n",
    "\n",
    "`B` is the batch size, `S` is the number of words and `E` is the embedding size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = torch.Tensor([[[0.2, 0.1, 0.3], [0.5, 0.1, 0.1]]])\n",
    "B, S, E = inputs.size()  # [1, 2, 3]\n",
    "inputs = inputs.reshape(S, B, E)\n",
    "\n",
    "inputs.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because now we have this batch dimension, layer normalisation is going to be applied to not just the layer but also across the batches. In this case, batch size is $1$ so it's not going to make much difference but layer normalisation is essentially going to be computed the layers and also the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3]), torch.Size([1, 3]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters_shape = inputs.size()[-2:]  # [1, 3]\n",
    "gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "beta = nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "gamma.size(), beta.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialise `gamma` to be just ones whereas `beta` will be just zeros. Then, we compute the dimensions for which we want to compute layer normalisation — the batch dimension as well as the embedding dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1, -2]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims = [-(i + 1) for i in range(len(parameters_shape))]\n",
    "\n",
    "dims"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`[-1, -2]` simply means the last two layers which in this case are the batch dimension as well as the embedding dimension. Now, we take the mean across these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2000]],\n",
       "\n",
       "        [[0.2333]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = inputs.mean(\n",
    "    dim=dims, keepdim=True\n",
    ")  # Taking the mean across the batch and embedding dimensions\n",
    "\n",
    "print(mean.size())\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0817]],\n",
       "\n",
       "        [[0.1886]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "epsilon = 1e-5  # For numerical stability\n",
    "std = (var + epsilon).sqrt()\n",
    "\n",
    "print(std.size())\n",
    "std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = (inputs - mean) / std\n",
    "\n",
    "print(y.size())\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000, -1.2238,  1.2238]],\n",
       "\n",
       "        [[ 1.4140, -0.7070, -0.7070]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = gamma * y + beta\n",
    "\n",
    "print(out.size())\n",
    "out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LayerNormalisation(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape = parameters_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(self.parameters_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(self.parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdims=True)\n",
    "        print(f\"Mean \\n({mean.shape}): \\n{mean}\")\n",
    "\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdims=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        print(f\"Standard Deviation \\n({std.shape}): \\n{std}\")\n",
    "\n",
    "        y = (inputs - mean) / std\n",
    "        print(f\"y \\n({y.shape}): \\n{y}\")\n",
    "\n",
    "        out = self.gamma * y + self.beta\n",
    "        print(f\"out \\n({out.shape}): \\n{out}\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs \n",
      "(torch.Size([5, 3, 8])) \n",
      "tensor([[[ 0.8309, -1.4533,  1.6044, -0.5960,  0.7909,  1.0906, -0.0633,\n",
      "          -1.0680],\n",
      "         [ 1.7214,  0.0474,  0.0555, -2.1137, -0.8513, -0.4475,  0.7492,\n",
      "          -0.9332],\n",
      "         [-0.5913, -1.0556,  0.4406, -0.4345,  0.9959, -1.5633,  0.4680,\n",
      "           0.3614]],\n",
      "\n",
      "        [[-0.8853,  0.3268,  1.1512, -0.0673, -1.1856,  1.2611, -0.9168,\n",
      "           1.5467],\n",
      "         [-0.8143, -0.8634,  1.2919,  0.7863,  1.8642,  0.6072,  0.7174,\n",
      "           0.1674],\n",
      "         [ 0.0502, -1.3253, -0.9979,  0.3378, -1.2231,  0.2320, -1.0998,\n",
      "           0.4230]],\n",
      "\n",
      "        [[ 0.7130,  1.3595,  0.0569,  2.2373, -1.5809, -0.7621,  0.4687,\n",
      "          -0.6500],\n",
      "         [-0.2873, -0.4107, -1.8128,  2.2755,  1.0251,  0.3668, -0.5424,\n",
      "          -1.5654],\n",
      "         [ 0.4478, -1.3341,  0.5928, -0.5933, -1.9637, -0.3187,  0.6185,\n",
      "           0.0860]],\n",
      "\n",
      "        [[-0.6113, -1.2800,  1.4437,  0.2136, -0.3700, -2.3754, -1.2255,\n",
      "          -1.3245],\n",
      "         [-0.2706,  0.0489,  1.3212,  1.8462, -0.6107, -1.3274,  1.5300,\n",
      "          -0.7590],\n",
      "         [ 1.0151,  1.8124,  0.5904,  0.7440,  0.2238, -1.7501,  0.6811,\n",
      "           0.5217]],\n",
      "\n",
      "        [[ 0.0099, -1.0649,  0.8170, -0.5438, -0.0278, -0.3871,  0.6067,\n",
      "           0.0638],\n",
      "         [-1.3476,  1.1779, -0.2881, -2.0713,  0.0444, -1.2334, -0.1371,\n",
      "          -0.0320],\n",
      "         [-0.2207,  0.9073, -0.6940,  0.2451,  0.4908,  1.1659, -0.0286,\n",
      "          -1.7392]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "sentence_length = 5\n",
    "embedding_dim = 8\n",
    "inputs = torch.randn(sentence_length, batch_size, embedding_dim)\n",
    "\n",
    "print(f\"inputs \\n({inputs.size()}) \\n{inputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_shape = inputs.size()[-2:]\n",
    "layer_norm = LayerNormalisation(parameters_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean \n",
      "(torch.Size([5, 1, 1])): \n",
      "tensor([[[-0.0839]],\n",
      "\n",
      "        [[ 0.0577]],\n",
      "\n",
      "        [[-0.0656]],\n",
      "\n",
      "        [[ 0.0037]],\n",
      "\n",
      "        [[-0.1786]]])\n",
      "Standard Deviation \n",
      "(torch.Size([5, 1, 1])): \n",
      "tensor([[[0.9989]],\n",
      "\n",
      "        [[0.9631]],\n",
      "\n",
      "        [[1.1303]],\n",
      "\n",
      "        [[1.1641]],\n",
      "\n",
      "        [[0.8420]]])\n",
      "y \n",
      "(torch.Size([5, 3, 8])): \n",
      "tensor([[[ 0.9158, -1.3709,  1.6902, -0.5126,  0.8758,  1.1758,  0.0207,\n",
      "          -0.9852],\n",
      "         [ 1.8073,  0.1315,  0.1396, -2.0319, -0.7682, -0.3639,  0.8341,\n",
      "          -0.8501],\n",
      "         [-0.5079, -0.9727,  0.5251, -0.3509,  1.0811, -1.4810,  0.5525,\n",
      "           0.4458]],\n",
      "\n",
      "        [[-0.9792,  0.2795,  1.1355, -0.1297, -1.2910,  1.2496, -1.0119,\n",
      "           1.5461],\n",
      "         [-0.9054, -0.9565,  1.2816,  0.7565,  1.8758,  0.5706,  0.6850,\n",
      "           0.1139],\n",
      "         [-0.0077, -1.4360, -1.0961,  0.2909, -1.3300,  0.1810, -1.2019,\n",
      "           0.3794]],\n",
      "\n",
      "        [[ 0.6888,  1.2608,  0.1083,  2.0374, -1.3407, -0.6163,  0.4727,\n",
      "          -0.5170],\n",
      "         [-0.1962, -0.3054, -1.5458,  2.0712,  0.9649,  0.3825, -0.4219,\n",
      "          -1.3269],\n",
      "         [ 0.4542, -1.1223,  0.5825, -0.4669, -1.6793, -0.2240,  0.6052,\n",
      "           0.1341]],\n",
      "\n",
      "        [[-0.5283, -1.1026,  1.2370,  0.1804, -0.3210, -2.0436, -1.0559,\n",
      "          -1.1409],\n",
      "         [-0.2356,  0.0389,  1.1318,  1.5828, -0.5278, -1.1434,  1.3111,\n",
      "          -0.6552],\n",
      "         [ 0.8689,  1.5537,  0.5040,  0.6360,  0.1891, -1.5065,  0.5819,\n",
      "           0.4450]],\n",
      "\n",
      "        [[ 0.2239, -1.0527,  1.1825, -0.4337,  0.1791, -0.2476,  0.9327,\n",
      "           0.2880],\n",
      "         [-1.3883,  1.6111, -0.1301, -2.2479,  0.2649, -1.2528,  0.0493,\n",
      "           0.1741],\n",
      "         [-0.0500,  1.2897, -0.6121,  0.5032,  0.7950,  1.5969,  0.1782,\n",
      "          -1.8534]]])\n",
      "out \n",
      "(torch.Size([5, 3, 8])): \n",
      "tensor([[[ 0.9158, -1.3709,  1.6902, -0.5126,  0.8758,  1.1758,  0.0207,\n",
      "          -0.9852],\n",
      "         [ 1.8073,  0.1315,  0.1396, -2.0319, -0.7682, -0.3639,  0.8341,\n",
      "          -0.8501],\n",
      "         [-0.5079, -0.9727,  0.5251, -0.3509,  1.0811, -1.4810,  0.5525,\n",
      "           0.4458]],\n",
      "\n",
      "        [[-0.9792,  0.2795,  1.1355, -0.1297, -1.2910,  1.2496, -1.0119,\n",
      "           1.5461],\n",
      "         [-0.9054, -0.9565,  1.2816,  0.7565,  1.8758,  0.5706,  0.6850,\n",
      "           0.1139],\n",
      "         [-0.0077, -1.4360, -1.0961,  0.2909, -1.3300,  0.1810, -1.2019,\n",
      "           0.3794]],\n",
      "\n",
      "        [[ 0.6888,  1.2608,  0.1083,  2.0374, -1.3407, -0.6163,  0.4727,\n",
      "          -0.5170],\n",
      "         [-0.1962, -0.3054, -1.5458,  2.0712,  0.9649,  0.3825, -0.4219,\n",
      "          -1.3269],\n",
      "         [ 0.4542, -1.1223,  0.5825, -0.4669, -1.6793, -0.2240,  0.6052,\n",
      "           0.1341]],\n",
      "\n",
      "        [[-0.5283, -1.1026,  1.2370,  0.1804, -0.3210, -2.0436, -1.0559,\n",
      "          -1.1409],\n",
      "         [-0.2356,  0.0389,  1.1318,  1.5828, -0.5278, -1.1434,  1.3111,\n",
      "          -0.6552],\n",
      "         [ 0.8689,  1.5537,  0.5040,  0.6360,  0.1891, -1.5065,  0.5819,\n",
      "           0.4450]],\n",
      "\n",
      "        [[ 0.2239, -1.0527,  1.1825, -0.4337,  0.1791, -0.2476,  0.9327,\n",
      "           0.2880],\n",
      "         [-1.3883,  1.6111, -0.1301, -2.2479,  0.2649, -1.2528,  0.0493,\n",
      "           0.1741],\n",
      "         [-0.0500,  1.2897, -0.6121,  0.5032,  0.7950,  1.5969,  0.1782,\n",
      "          -1.8534]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out = layer_norm(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
